***

## æ¨¡å‹ä»‹ç»

![](images/801988cd-5f01-45eb-a144-e4f11ac359b1.png)

Qwen2VLæ˜¯ç”±é˜¿é‡Œå›¢é˜Ÿå¼€æºçš„æœ€æ–°å›¾åƒã€è§†é¢‘è¯†åˆ«çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ”¯æŒå¯¹å„ç§åˆ†è¾¨ç‡å’Œæ¯”ä¾‹çš„å›¾åƒçš„ç†è§£å¹¶è¿”å›æ–‡æœ¬å¯¹è¯ï¼Œå¹¶å…·æœ‰åŸºæœ¬çš„ç»“åˆè§†è§‰çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ã€æ•°å­¦æ¨ç†èƒ½åŠ›ã€å¤šè¯­è¨€èƒ½åŠ›ã€‚Qwen2VLå¯ä»¥ç†è§£ 20 åˆ†é’Ÿä»¥ä¸Šçš„è§†é¢‘ï¼Œä»¥è¿›è¡Œé«˜è´¨é‡çš„åŸºäºè§†é¢‘çš„é—®ç­”ã€å¯¹è¯ã€å†…å®¹åˆ›å»ºç­‰ã€‚

é™¤äº†è‹±æ–‡å’Œä¸­æ–‡å¤–ï¼ŒQwen2-VL ç°åœ¨è¿˜æ”¯æŒç†è§£å›¾åƒä¸­ä¸åŒè¯­è¨€çš„æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¤§å¤šæ•°æ¬§æ´²è¯­è¨€ã€æ—¥è¯­ã€éŸ©è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€è¶Šå—è¯­ç­‰ã€‚

æœ¬æ¬¡å¼€æºçš„æ¨¡å‹åŒ…æ‹¬2Bã€7Bã€72Bå‚æ•°çš„æ¨¡å‹ï¼Œè¿è¡Œå¯¹åº”æ¨¡å‹å¤§è‡´éœ€è¦çš„æ˜¾å­˜åˆ†åˆ«è‡³å°‘ä¸º6Gã€34Gã€144Gã€‚

* å›¾åƒèƒ½åŠ›æµ‹è¯•

![](images/a8e3828e-3407-4cb0-a74b-18a92ebdf61f.png)

* è§†é¢‘èƒ½åŠ›æµ‹è¯•

![](images/29261904-c15e-40ab-a58a-bc79e3a9d719.png)

## æœ¬åœ°éƒ¨ç½²æµç¨‹

* **Step 1. åˆ›å»ºcondaè™šæ‹Ÿç¯å¢ƒ**

â€ƒâ€ƒCondaåˆ›å»ºè™šæ‹Ÿç¯å¢ƒçš„æ„ä¹‰åœ¨äºæä¾›äº†ä¸€ä¸ªéš”ç¦»çš„ã€ç‹¬ç«‹çš„ç¯å¢ƒï¼Œç”¨äºPythoné¡¹ç›®å’Œå…¶ä¾èµ–åŒ…çš„ç®¡ç†ã€‚æ¯ä¸ªè™šæ‹Ÿç¯å¢ƒéƒ½æœ‰è‡ªå·±çš„Pythonè¿è¡Œæ—¶å’Œä¸€ç»„åº“ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨ä¸åŒçš„ç¯å¢ƒä¸­å®‰è£…ä¸åŒç‰ˆæœ¬çš„åº“è€Œäº’ä¸å½±å“ã€‚æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¿¡æ¯å»ºè®®Pythonç‰ˆæœ¬3.10ä»¥ä¸Šã€‚åˆ›å»ºè™šæ‹Ÿç¯å¢ƒçš„åŠæ³•å¯ä»¥é€šè¿‡ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºï¼š

```bash
# qwen2_5 æ˜¯ä½ æƒ³è¦ç»™ç¯å¢ƒçš„åç§°ï¼Œpython=3.11 æŒ‡å®šäº†è¦å®‰è£…çš„Pythonç‰ˆæœ¬ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©ä¸åŒçš„åç§°å’Œ/æˆ–Pythonç‰ˆæœ¬ã€‚

conda create -n qwen2_5 python=3.11
```

![](images/32c981b5-bb09-4243-ba84-005474f7a657.png)

â€ƒâ€ƒåˆ›å»ºè™šæ‹Ÿç¯å¢ƒåï¼Œéœ€è¦æ¿€æ´»å®ƒã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æ¿€æ´»åˆšåˆšåˆ›å»ºçš„ç¯å¢ƒã€‚å¦‚æœæˆåŠŸæ¿€æ´»ï¼Œå¯ä»¥çœ‹åˆ°åœ¨å‘½ä»¤è¡Œçš„æœ€å‰æ–¹çš„æ‹¬å·ä¸­ï¼Œå°±æ ‡è¯†äº†å½“å‰çš„è™šæ‹Ÿç¯å¢ƒã€‚è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå®Œæˆåæ¥ä¸‹æ¥å®‰è£…torchã€‚

![](images/3c944e1b-6ff7-46fd-bc4a-8639a99b1873.png)

â€ƒâ€ƒå¦‚æœå¿˜è®°æˆ–è€…æƒ³è¦ç®¡ç†è‡ªå·±å»ºç«‹çš„è™šæ‹Ÿç¯å¢ƒï¼Œå¯ä»¥é€šè¿‡conda env listå‘½ä»¤æ¥æŸ¥çœ‹æ‰€æœ‰å·²åˆ›å»ºçš„ç¯å¢ƒåç§°ã€‚

![](images/cdd8d11e-9ec9-45b1-82d5-c1c3f94b4db6.png)

â€ƒâ€ƒå¦‚æœéœ€è¦å¸è½½æŒ‡å®šçš„è™šæ‹Ÿç¯å¢ƒåˆ™é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤å®ç°ï¼š

```plaintext
conda env remove --name envname
```

![](images/95d13ecb-e9ae-4f96-aa75-296ea86f8330.png)

* éœ€è¦æ³¨æ„çš„æ˜¯æ— æ³•å¸è½½å½“å‰æ¿€æ´»çš„ç¯å¢ƒï¼Œå»ºè®®å¸è½½æ—¶å…ˆåˆ‡æ¢åˆ°baseç¯å¢ƒä¸­å†æ‰§è¡Œæ“ä½œã€‚

* **Step 2. æŸ¥çœ‹å½“å‰é©±åŠ¨æœ€é«˜æ”¯æŒçš„CUDAç‰ˆæœ¬**

â€ƒâ€ƒæˆ‘ä»¬éœ€è¦æ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©Pytorchæ¡†æ¶ï¼Œå…ˆçœ‹ä¸‹å½“å‰çš„CUDAç‰ˆæœ¬ï¼š

```plaintext
nvidia -smi
```

![](images/a5f649f5-c59c-46ad-bd8e-1718e9f9a0b9.png)

* **Step 3. åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…Pytorch**

â€ƒâ€ƒè¿›å…¥Pytorchå®˜ç½‘ï¼šhttps://pytorch.org/get-started/previous-versions/

![](images/a96c5f11-ebf4-482c-bdaf-72423fa1eee3.png)

â€ƒâ€ƒå½“å‰çš„ç”µè„‘CUDAçš„æœ€é«˜ç‰ˆæœ¬è¦æ±‚æ˜¯12.2ï¼Œæ‰€ä»¥éœ€è¦æ‰¾åˆ°ä¸å¤§äº12.2ç‰ˆæœ¬çš„Pytorchã€‚

![](images/9de08448-61b8-4cc0-be67-b0dfdf6e510c.png)

â€ƒâ€ƒç›´æ¥å¤åˆ¶å¯¹åº”çš„å‘½ä»¤ï¼Œè¿›å…¥ç»ˆç«¯æ‰§è¡Œå³å¯ã€‚è¿™å®é™…ä¸Šå®‰è£…çš„æ˜¯ä¸º CUDA 12.1 ä¼˜åŒ–çš„ PyTorch ç‰ˆæœ¬ã€‚è¿™ä¸ª PyTorch ç‰ˆæœ¬é¢„ç¼–è¯‘å¹¶æ‰“åŒ…äº†ä¸ CUDA 12.1 ç‰ˆæœ¬ç›¸å¯¹åº”çš„äºŒè¿›åˆ¶æ–‡ä»¶å’Œåº“ã€‚

![](images/d9060131-753e-471b-8b6b-ac89513afedf.png)

* **Step 4. å®‰è£…å¿…è¦çš„ä¾èµ–**

Transfomersæ˜¯å¤§æ¨¡å‹æ¨ç†æ—¶æ‰€éœ€è¦ä½¿ç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹æŒ‡ä»¤å¯ä»¥ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„Transfomersï¼š

pip install transformers -U

![](images/ad19047d-54c4-4309-88fe-62ceb3d207a0.png)

å®‰è£…å®Œæˆåå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥æ˜¯å¦å®‰è£…ï¼š

```plaintext
pip show transformers
```

![](images/c6b10f05-fd3a-4a76-b7fd-cf764abfa8c5.png)

æ¥ä¸‹æ¥éœ€è¦å®‰è£…ä¸‹è½½å·¥å…·modelscopeä»¥åŠæ¥ä¸‹æ¥è¦ä¸‹è½½è„šæœ¬çš„ä¾èµ–accelerateï¼Œé€šè¿‡ä»¥ä¸‹ä»£ç è¿›è¡Œå¯¹åº”å·¥å…·çš„éƒ¨ç½²ï¼š

pip install modelscope

![](images/9d4d7459-c652-47e8-a26b-b839f9b2598e.png)

pip install accelerate>=0.26.0

![](images/771de421-0c73-4052-827c-01d51f2c495a.png)

å®‰è£…qwen\_vl\_utilsåº“æ¥åŠ é€Ÿæ¨¡å‹æ¨ç†ï¼š

pip install qwen-vl-utils

![](images/3be2b302-6361-4473-a73e-65f50aa617fe.png)

* **Step 5. ä¸‹è½½æ¨¡å‹æƒé‡**

åœ¨ä¸‹è½½æ¨¡å‹æƒé‡ä¹‹å‰ï¼Œå…ˆä¸ºä¹‹åˆ›å»ºåˆé€‚çš„æ–‡ä»¶å¤¹ã€‚

![](images/90a729b6-88d3-4059-9ac8-92344c499b16.png)

æœ¬æµç¨‹é€‰æ‹©åœ¨modelscopeå¹³å°ä¸‹è½½Qwen2VLæ¨¡å‹ï¼Œè¿™ç§æ–¹å¼åœ¨å›½å†…å®‰è£…éƒ¨ç½²å¤§æ¨¡å‹éå¸¸ä¾¿æ·ã€‚é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¼€å§‹ä¸‹è½½ï¼Œéœ€æ³¨æ„æœ€åçš„ä¸€ä¸ªå‚æ•°è¦ä¿®æ”¹æˆä½ è¦å­˜æ”¾æ¨¡å‹çš„æ–‡ä»¶å¤¹ã€‚

modelscope download --model Qwen/Qwen2-VL-7B-Instruct --local\_dir ./dir

![](images/b81a7f86-9cd8-4a8e-85d5-e663234dd808.png)

## æœ¬åœ°è°ƒç”¨æµ‹è¯•

#### å•å›¾æ¨ç†

* å¯¼å…¥ç›¸å…³åº“

```python
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from modelscope import snapshot_download
```

* è®¾ç½®æ¨¡å‹è·¯å¾„

```python
model_dir = './model/dir'
```

* å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡

```python

model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_dir, torch_dtype="auto", device_map="auto"

processor = AutoProcessor.from_pretrained(model_dir)
)
```

* åˆ›å»ºæ¶ˆæ¯

```python
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
```

* è¿›è¡Œæ¨ç†

```python

generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

demoä¸­è¿›è¡Œæµ‹è¯•çš„å›¾ç‰‡å†…å®¹å¦‚ä¸‹ï¼š

![](images/06da3798-71e4-49c1-a26b-d6780b94b387.png)

æ‰§è¡Œå®Œè¿”å›çš„ç»“æœä¼šå¦‚ä¸‹ï¼š

![](images/b4218ec9-f01f-491e-b3d7-c0aa86103eb3.png)

è¿›è¡Œæ¨ç†Qwen2VLï¼š7Bè¯†åˆ«å®˜æ–¹ç»™å‡ºçš„demoå›¾åƒæ¶ˆè€—å°†è¿‘34Gæ˜¾å­˜ã€‚

![](images/da975ac5-f1d8-4c89-a1c6-6e2ca6f77e75.png)

å¯¹äºå…¶ä»–çš„ä¿¡æ¯æºæµ‹è¯•ï¼šæ— è®ºæ˜¯äº‘å¹³å°æŒ‚è½½çš„å›¾ç‰‡è¿˜æ˜¯æœ¬åœ°å›¾ç‰‡è¯†åˆ«ï¼Œæ¶ˆè€—çš„æ˜¾å­˜éƒ½ä¸è¶…è¿‡16Gã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå±•ç¤ºæ¨¡å‹æ€§èƒ½çš„å›¾è¡¨ï¼Œå°†ä¹‹ç”¨ä¸Šè¿°æ–¹æ³•äº¤ä»˜ç»™Qwen2VLè¿›è¡Œæ¨ç†ï¼š

![](images/a6cfd4cb-edc2-4acd-a35d-599479b25a2e.png)

æ¨ç†è¿”å›ç»“æœå¦‚ä¸‹

![](images/fd1cd4ef-effd-4515-9ca5-6b78261d0bf8.png)

æ˜¾å­˜æ¶ˆè€—æƒ…å†µå¦‚ä¸‹ï¼Œå¤§çº¦æ¶ˆè€—16Gã€‚

![](images/5885d86e-4315-4e59-81f4-cdb0e39b80a7.png)

#### å¤šå›¾æ¨ç†

å¤šå›¾æ¨ç†çš„ä»£ç å¦‚ä¸‹ï¼Œéœ€æ³¨æ„ä¿®æ”¹æœ¬åœ°å¯åŠ¨æ¨¡å‹è·¯å¾„å’Œä¼ å…¥ä¿¡æ¯ä¸­çš„å›¾åƒä¿¡æ¯è·¯å¾„ï¼Œå…¶ä¸­ï¼Œä¼ å…¥è¿›è¡Œæ¨ç†çš„å›¾åƒæ”¯æŒæœ¬åœ°.jpg / .png /.jpeg æ ¼å¼çš„æ–‡ä»¶ï¼Œä¹Ÿæ”¯æŒçº¿ä¸Šurlæ ¼å¼çš„æ–‡ä»¶ã€‚

* å¤šå›¾æ¨ç†çš„ä»£ç å¦‚ä¸‹

```python
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from modelscope import snapshot_download
model_dir = '/home/LLM/qwen2VL'

model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_dir, torch_dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained(model_dir)

# Messages containing multiple images and a text query
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "/home/LLM/llama3_2_v/mao.png"},
            {"type": "image", "image": "/home/LLM/llama3_2_v/fm.png"},
            {"type": "text", "text": "Identify the similarities between these images."},
        ],
    }
]

# Preparation for inference
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to("cuda")

# Inference
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

![](images/5a0b997d-5439-4930-adf0-8b6c27fd5fbf.png)

![](images/2787c1c6-bfe9-4bdc-a4eb-18738c020f94.png)

ä¼ å…¥çš„å›¾ç‰‡ä¸€ä¸ªæ˜¯.jpgçš„ï¼Œä¸€ä¸ªæ˜¯.pngçš„ï¼Œè¿”å›å†…å®¹å¦‚ä¸‹ï¼š

![](images/cc7c467a-f2f3-4c7b-9aa4-612d1d1da79d.png)

Qwen2VLä¹Ÿæ”¯æŒä¸­æ–‡é—®ç­”.

![](images/5834beb2-b636-4bb2-8377-16df0018161c.png)

#### è§†é¢‘æ¨ç†

```python
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from modelscope import snapshot_download
model_dir = '/home/LLM/qwen2VL'

model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_dir, torch_dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained(model_dir)


messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "/home/LLM/llama3_2_v/å¯çµ-å°è€è™.mp4",
                "max_pixels": 360 * 420,
                "fps": 1.0,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Preparation for inference
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to("cuda")

# Inference
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

æ¨ç†æ•ˆæœå¦‚ä¸‹ï¼š

![](images/dc33e725-dfa4-40e1-80c8-307ee9a587a2.png)

è¿”å›ä¸­æ–‡æ¨ç†ç»“æœï¼š

![](images/a665d77f-4e3e-4edd-8fd8-49d4a11c5359.png)

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè§†é¢‘ä»…æ”¯æŒè¾“å…¥æœ¬åœ°è·¯å¾„çš„æ–‡ä»¶è¿›è¡Œæ¨ç†ã€‚

å°½ç®¡ Qwen2-VL åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»ç¬¬ä¸‰æ–¹çš„è§†è§’æ¥çœ‹ï¼Œå®ƒä»ç„¶å­˜åœ¨ä¸€äº›å€¼å¾—æ³¨æ„çš„å±€é™æ€§ï¼Œè¿™äº›é™åˆ¶å¯¹å…¶åœ¨ç‰¹å®šåœºæ™¯ä¸­çš„é€‚ç”¨æ€§æå‡ºäº†æŒ‘æˆ˜ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¸»è¦çš„è§‚å¯Ÿç‚¹ï¼š

* éŸ³é¢‘æ”¯æŒç¼ºå¤±
  Qwen2-VL ç›®å‰æ— æ³•å¤„ç†è§†é¢‘ä¸­çš„éŸ³é¢‘ä¿¡æ¯ï¼Œå› æ­¤åœ¨éœ€è¦ç»¼åˆè§†è§‰ä¸å¬è§‰çš„ä»»åŠ¡ä¸­ï¼Œå…¶è¡¨ç°å¯èƒ½å—é™ã€‚

* æ•°æ®è¦†ç›–çš„æ—¶æ•ˆæ€§
  æ¨¡å‹çš„å›¾åƒè®­ç»ƒæ•°æ®æ›´æ–°è‡³ 2023 å¹´ 6 æœˆï¼Œä¹‹åçš„äº‹ä»¶æˆ–å˜åŒ–å¯èƒ½æœªè¢«çº³å…¥ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ—¶æ•ˆæ€§è¦æ±‚è¾ƒé«˜åœºæ™¯ä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚

* ä¸ªäººä¸çŸ¥è¯†äº§æƒè¯†åˆ«èƒ½åŠ›çš„å±€é™
  åœ¨è¯†åˆ«ç‰¹å®šäººç‰©æˆ–å“ç‰Œæ–¹é¢ï¼Œæ¨¡å‹å¯èƒ½å­˜åœ¨ç›²ç‚¹ï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒå°‘æ›å…‰çš„ä¸ªäººæˆ–å†·é—¨çŸ¥è¯†äº§æƒçš„è¯†åˆ«ï¼Œå‡†ç¡®ç‡ä»éœ€æå‡ã€‚

* å¤æ‚æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›
  å½“ç”¨æˆ·æå‡ºæ¶‰åŠå¤šä¸ªæ­¥éª¤æˆ–éœ€è¦å¤æ‚é€»è¾‘çš„æŒ‡ä»¤æ—¶ï¼ŒQwen2-VL çš„ç†è§£ä¸æ‰§è¡Œèƒ½åŠ›è¡¨ç°å‡ºä¸€å®šä¸è¶³ï¼Œéš¾ä»¥é«˜æ•ˆå®Œæˆå¤æ‚ä»»åŠ¡ã€‚

* è®¡æ•°èƒ½åŠ›ä¸è¶³
  ç‰¹åˆ«æ˜¯åœ¨åŒ…å«å¤§é‡ç‰©ä½“æˆ–å¤æ‚èƒŒæ™¯çš„åœºæ™¯ä¸­ï¼Œæ¨¡å‹å¯¹ç‰©ä½“æ•°é‡çš„è¯†åˆ«å¾€å¾€ä¸å¤Ÿç²¾å‡†ï¼Œè¿™å¯èƒ½å¯¹ç‰¹å®šåº”ç”¨åœºæ™¯ï¼ˆå¦‚ç‰©æµæˆ–ç‰©å“æ¸…ç‚¹ï¼‰äº§ç”Ÿå½±å“ã€‚

* ç©ºé—´æ¨ç†èƒ½åŠ›çš„çŸ­æ¿
  æ¨¡å‹åœ¨æ¨æ–­ 3D ç©ºé—´ä¸­ç‰©ä½“çš„ç›¸å¯¹ä½ç½®æ—¶è¡¨ç°è¾ƒå¼±ï¼Œå¯¹äºéœ€è¦ç²¾ç¡®ç©ºé—´ç†è§£çš„ä»»åŠ¡ï¼ˆå¦‚å»ºç­‘è®¾è®¡æˆ–æœºå™¨äººå¯¼èˆªï¼‰ä»æœ‰è¾ƒå¤§æ”¹è¿›ç©ºé—´ã€‚

è¿™äº›å±€é™æ€§è¡¨æ˜ï¼Œå°½ç®¡ Qwen2-VL å±•ç°å‡ºå¼ºå¤§çš„è§†è§‰èƒ½åŠ›ï¼Œä½†å…¶æ€§èƒ½åœ¨æŸäº›ç‰¹å®šé¢†åŸŸä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚æœªæ¥è‹¥èƒ½é’ˆå¯¹è¿™äº›çŸ­æ¿è¿›è¡ŒæŒç»­æ”¹è¿›ï¼Œæ¨¡å‹çš„åº”ç”¨èŒƒå›´å’Œå®ç”¨æ€§å°†è¿›ä¸€æ­¥æ‹“å®½ã€‚

### æ¨ç†Qwen2vlï¼š2bæ¨¡å‹

```python

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

model_dir = '/home/LLM/qwen2vl-2b'

# åŠ è½½æ¨¡å‹åˆ° GPU0
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_dir, torch_dtype="auto"
)
device = torch.device("cuda:0")
model = model.to(device)

# åŠ è½½å¤„ç†å™¨
processor = AutoProcessor.from_pretrained(model_dir)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# å‡†å¤‡æ–‡æœ¬å’Œè§†è§‰è¾“å…¥
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)

inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)

# ç¡®ä¿ inputs åœ¨ GPU0 ä¸Š
inputs = inputs.to(device)

# æ¨ç†ï¼šç”Ÿæˆè¾“å‡º
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

* å®˜æ–¹demoæ¼”ç¤º

![](images/06da3798-71e4-49c1-a26b-d6780b94b387-1.png)

![](images/b7eaf8b8-8a32-4573-8875-75745c001907.png)

æ¨ç†çº¿ä¸Šå›¾ç‰‡éœ€è¦å°†è¿‘19Gæ˜¾å­˜ã€‚

![](images/5b4957f9-7781-4e7e-acc7-76f0e08e0b4a.png)

* æœ¬åœ°å›¾ç‰‡è¯†åˆ«æµ‹è¯•

![](images/e30c0fef-642b-4e15-9d56-1a98b6273581.png)

![](images/235f84c4-8e54-49db-b2cb-542bc39c937e.png)

![](images/691a0ee7-4af5-4105-ab0d-69ef6534f5e5.png)

æ¨ç†å¤§æ¦‚éœ€è¦6Gçš„æ˜¾å­˜æ¶ˆè€—ã€‚

![](images/7bc1e044-4303-4c45-96cc-b052fa6e7541.png)



***

ğŸ“**æ›´å¤šå¤§æ¨¡å‹æŠ€æœ¯å†…å®¹å­¦ä¹ **

**æ‰«ç æ·»åŠ åŠ©ç†è‹±è‹±ï¼Œå›å¤â€œå¤§æ¨¡å‹â€ï¼Œäº†è§£æ›´å¤šå¤§æ¨¡å‹æŠ€æœ¯è¯¦æƒ…å“¦ğŸ‘‡**

![](images/f339b04b7b20233dd1509c7fb36d5c0.png)

æ­¤å¤–ï¼Œ**æ‰«ç å›å¤â€œå…¥ç¾¤â€**ï¼Œå³å¯åŠ å…¥**å¤§æ¨¡å‹æŠ€æœ¯ç¤¾ç¾¤ï¼šæµ·é‡ç¡¬æ ¸ç‹¬å®¶æŠ€æœ¯`å¹²è´§å†…å®¹`+æ— é—¨æ§›`æŠ€æœ¯äº¤æµ`ï¼**
