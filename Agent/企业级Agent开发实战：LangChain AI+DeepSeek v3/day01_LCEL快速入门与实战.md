&emsp;&emsp;ä½“éªŒè¯¾æ—¶é—´æœ‰é™ï¼Œè‹¥æƒ³æ·±åº¦å­¦ä¹ å¤§æ¨¡å‹æŠ€æœ¯ï¼Œæ¬¢è¿å¤§å®¶æŠ¥åç”±æˆ‘ä¸»è®²çš„[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹](https://whakv.xetslk.com/s/3xFEAA)ï¼š

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/a4f3bdfb511aab72c83ee7d1fbefeff.png" alt="a4f3bdfb511aab72c83ee7d1fbefeff" style="zoom:30%;" />

**[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹](https://whakv.xetslk.com/s/3xFEAA)ä¸ºã€100+å°æ—¶ã€‘ä½“ç³»å¤§è¯¾ï¼Œæ€»å…±20å¤§æ¨¡å—ç²¾è®²ç²¾æï¼Œé›¶åŸºç¡€ç›´è¾¾å¤§æ¨¡å‹ä¼ä¸šçº§åº”ç”¨ï¼**

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/bbf269a0de3abba3e4ee21b29bc7cfc.png" alt="bbf269a0de3abba3e4ee21b29bc7cfc" style="zoom:50%;" />

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/eb29b5d74f0b1d86aea1b5698f6ea9d.png" alt="eb29b5d74f0b1d86aea1b5698f6ea9d" style="zoom:50%;" />

**[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹](https://whakv.xetslk.com/s/3xFEAA)2025å¹´æ–°æ˜¥ç­ä¸Šæ–°ç‰¹æƒ ï¼Œæ—©é¸Ÿä»·å…¥å­¦ï¼Œç›´é™2000ï¼Œæœ¨ç¾½è€å¸ˆç›´æ’­é—´ä¸“å±å åŠ ä¼˜æƒ åˆ¸1000ï¼Œä»…éœ€2999æŠ¥åå³å­¦ï¼Œã€ä»…é™å‰10åã€‘<span style="color:red;">è¯¦ç»†ä¿¡æ¯æ‰«ç æ·»åŠ åŠ©æ•™ï¼Œå›å¤â€œå¤§æ¨¡å‹â€ï¼Œå³å¯é¢†å–è¯¾ç¨‹å¤§çº²&æŸ¥çœ‹è¯¾ç¨‹è¯¦æƒ…ğŸ‘‡</span>**

<center><img src="https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202501131753098.png" alt="c6847a817fd7efd0cddcb1bcac217c3" style="zoom:85%;" />

<center><img src="https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202501141145823.png" alt="image-20250107200452887" style="zoom:85%;" />

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107200502389.png" alt="image-20250107200502389" style="zoom:85%;" />

---

# <center> ã€Š2025å¤§æ¨¡å‹æ™ºèƒ½ä½“Agentå¼€å‘å®æˆ˜ã€‹ä½“éªŒè¯¾         
## <center> LangChain AI+ DeepSeek v3 ä¼ä¸š Agent å¼€å‘å®æˆ˜
## <center> Part 1. LCEL (LangChain Expression Language) å¿«é€Ÿå…¥é—¨ä¸å®æˆ˜

# 1. LCELåŸºæœ¬æ¦‚å¿µå…¥é—¨

&emsp;&emsp;`LCEL(LangChain Expression Language)`æ˜¯ `LangChain` ä¸­éå¸¸å…³é”®çš„æ¦‚å¿µï¼Œå®ƒæŒ‡çš„æ˜¯ä¸€ç§å£°æ˜å¼çš„è¡¨è¾¾å¼è¯­è¨€ï¼Œç”¨äºé€šè¿‡é“¾å¼ç»„åˆä¸åŒçš„æ¨¡å—ã€‚å®ƒå°†ä¸åŒçš„ç»„ä»¶é€šè¿‡ç»Ÿä¸€çš„`Runable`æ¥å£è¿æ¥èµ·æ¥ï¼Œä»è€Œå®ç°å…·ä½“çš„æµç¨‹æˆ–åŠŸèƒ½ï¼ˆæ¯”å¦‚RAGã€Agentï¼‰ã€‚

<div align=center><img src="https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202501131633573.png" width=80%></div>

&emsp;&emsp;å› ä¸ºæ¯ä¸ªæ¨¡å—éƒ½æ˜¯`LCEL`å¯¹è±¡ï¼Œå› æ­¤åŸºäº`LCEL`å¯¹è±¡è¿æ¥å½¢æˆçš„é“¾ï¼Œæœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ª `LCEL` å¯¹è±¡ï¼Œæ‰€ä»¥å½“é€šè¿‡ä¸€ç»„é€šç”¨çš„è°ƒç”¨æ–¹æ³•ï¼ˆinvokeã€ batchã€streamã€ ainvoke ï¼‰æ–¹æ³•æ—¶ï¼Œå°±èƒ½å¤Ÿåšåˆ°å®šåˆ¶åŒ–ç»„åˆé“¾ã€å¹¶è¡ŒåŒ–ç»„ä»¶ã€å›é€€ã€åŠ¨æ€é…ç½®é“¾å†…éƒ¨ç»“æ„ç­‰æ ‡å‡†åŒ–çš„æ“ä½œã€‚å®ƒçš„ä¼˜åŠ¿éå¸¸æ˜æ˜¾ï¼š
1. ç»Ÿä¸€çš„æ¥å£ï¼šæ¯ä¸ª `LCEL` å¯¹è±¡éƒ½å®ç°`Runnable`æ¥å£ï¼Œå› æ­¤å¯ä»¥éå¸¸æ–¹ä¾¿çš„è¿æ¥åˆ°ä¸€èµ·ï¼›
2. æ¨¡å—åŒ–æ“ä½œï¼šæ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•ï¼Œå¤„ç†å¥½è¾“å…¥å’Œè¾“å‡ºå°±å¯ä»¥é€šè¿‡`LCEL`é›†æˆåˆ°ä¸€èµ·ï¼›
3. è‰¯å¥½æ‰©å±•æ€§ï¼šå„ä¸ªæ¨¡å—ç»„ä»¶ä¹‹é—´éƒ½å®ç°äº†é€šç”¨çš„è°ƒç”¨æ–¹æ³•ï¼ˆinvokeã€ batchã€streamã€ ainvoke ï¼‰æ–¹æ³•ï¼Œå› æ­¤å¯ä»¥çµæ´»ç»„åˆä½¿ç”¨ä¸åŒçš„ä½¿ç”¨åœºæ™¯ï¼›

&emsp;&emsp;æ¯”å¦‚`LangChain`ä¸­æŠ½è±¡å‡ºæ¥çš„æœ€ç®€å•çš„ Model I/O æ¨¡å—ã€‚

<div align=center><img src="https://snowball101.oss-cn-beijing.aliyuncs.com/img/202403041734638.png" width=100%></div>

&emsp;&emsp;LangChainçš„Model I/Oæ¨¡å—æä¾›äº†æ ‡å‡†çš„ã€å¯æ‰©å±•çš„æ¥å£å®ç°ä¸å¤§è¯­è¨€æ¨¡å‹çš„å¤–éƒ¨é›†æˆã€‚æ‰€è°“çš„Model I/Oï¼ŒåŒ…æ‹¬æ¨¡å‹è¾“å…¥ï¼ˆPromptsï¼‰ã€æ¨¡å‹è¾“å‡ºï¼ˆOutPutsï¼‰å’Œæ¨¡å‹æœ¬èº«ï¼ˆModelsï¼‰ï¼Œç®€å•ç†è§£å°±æ˜¯é€šè¿‡è¯¥æ¨¡å—ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿä¸æŸä¸ªå¤§æ¨¡å‹è¿›è¡Œå¯¹è¯äº¤äº’ï¼Œæ•´ä¸ªå†…éƒ¨é€»è¾‘å°±ç›¸å½“äºæˆ‘ä»¬æœ€ç†Ÿæ‚‰çš„è¿™ä¸ªè¿‡ç¨‹ï¼šè¾“å…¥Promptï¼Œå¾—åˆ°å¤§æ¨¡å‹é’ˆå¯¹è¯¥Promptçš„æ¨ç†ç»“æœã€‚å¦‚ä¸‹ç¤ºä¾‹ä¸ºOpenAIçš„ GPT ç³»åˆ—æ¨¡å‹çš„API è°ƒç”¨è§„èŒƒï¼š

```python
response = openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": "è¯·é—®ï¼Œä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
  ]
)
```

&emsp;&emsp;åœ¨LangChainçš„Model I/Oæ¨¡å—è®¾è®¡ä¸­ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼š Prompt Templateï¼ˆå¯¹åº”ä¸Šå›¾ä¸­çš„Formatéƒ¨åˆ†ï¼‰ï¼Œ Modelï¼ˆå¯¹åº”ä¸Šå›¾ä¸­çš„Predictéƒ¨åˆ†ï¼‰ å’ŒOutput Parserï¼ˆå¯¹åº”ä¸Šå›¾ä¸­çš„Parseéƒ¨åˆ†ï¼‰ã€‚

- **Formatï¼šå³æŒ‡ä»£Prompts Templateï¼Œé€šè¿‡æ¨¡æ¿åŒ–æ¥ç®¡ç†å¤§æ¨¡å‹çš„è¾“å…¥ï¼›**
- **Predictï¼šå³æŒ‡ä»£Modelsï¼Œä½¿ç”¨é€šç”¨æ¥å£è°ƒç”¨ä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹ï¼›**
- **Parseï¼šå³æŒ‡ä»£Outputéƒ¨åˆ†ï¼Œç”¨æ¥ä»æ¨¡å‹çš„æ¨ç†ä¸­æå–ä¿¡æ¯ï¼Œå¹¶æŒ‰ç…§é¢„å…ˆè®¾å®šå¥½çš„æ¨¡ç‰ˆæ¥è§„èŒƒåŒ–è¾“å‡ºã€‚**

- **Format**

&emsp;&emsp;å¯¹äºPrompt Templateç¬¬ä¸€éƒ¨åˆ†ï¼Œä¼ ç»Ÿä¸Šæˆ‘ä»¬åˆ›å»ºæç¤ºè¯æ˜¯é€šè¿‡æ‰‹å·¥ç¼–å†™æ¥å®ç°çš„ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¼šåˆ©ç”¨å„ç§æç¤ºå·¥ç¨‹æŠ€å·§ï¼Œå¦‚Few-Shotã€é“¾å¼æ¨ç†ï¼ˆCoTï¼‰ç­‰æ–¹æ³•ï¼Œä»¥æé«˜å¤§æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚ç„¶è€Œï¼Œ**åœ¨åº”ç”¨å¼€å‘ä¸­ï¼Œä¸€ä¸ªå…³é”®çš„è€ƒé‡æ˜¯æç¤ºè¯ä¸èƒ½æ˜¯ä¸€æˆä¸å˜çš„ã€‚** å…¶åŸå› åœ¨äºï¼Œåº”ç”¨å¼€å‘éœ€è¦é€‚åº”å¤šå˜çš„ç”¨æˆ·éœ€æ±‚å’Œåœºæ™¯ã€‚å›ºå®šçš„æç¤ºè¯é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚ç”¨èŒƒå›´ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨å¼€å‘ä¸€ä¸ªå¤©æ°”æŸ¥è¯¢åº”ç”¨ï¼Œç”¨æˆ·å¯èƒ½ä¼šä»¥å¤šç§æ–¹å¼æå‡ºæŸ¥è¯¢ï¼Œå¦‚â€œä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿâ€æˆ–â€œæ˜å¤©çº½çº¦çš„æ¸©åº¦æ˜¯å¤šå°‘åº¦ï¼Ÿâ€ã€‚å¦‚æœæç¤ºè¯æ˜¯å›ºå®šçš„ï¼Œå®ƒå¯èƒ½åªèƒ½å¤„ç†ä¸€ç§ç‰¹å®šç±»å‹çš„æŸ¥è¯¢ï¼Œè€Œæ— æ³•é€‚åº”è¿™ç§å¤šæ ·æ€§çš„éœ€æ±‚ã€‚

&emsp;&emsp;è€ŒPrompt Templateï¼Œå°±åƒReActæ¨¡ç‰ˆï¼Œå°†APIçš„ä½¿ç”¨ã€é—®é¢˜è§£ç­”è¿‡ç¨‹ç­‰å¤æ‚é€»è¾‘å°è£…æˆäº†ä¸€å¥—ç»“æ„åŒ–çš„æ ¼å¼ã€‚æˆ‘ä»¬åªéœ€å‡†å¤‡å…·ä½“çš„å¤–éƒ¨å‡½æ•°ä¿¡æ¯å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œå³å¯ç”Ÿæˆå®šåˆ¶åŒ–çš„æç¤ºè¯ï¼Œå¼•å¯¼æ¨¡å‹æŒ‰ç…§æ—¢å®šé€»è¾‘è¿›è¡Œæ€è€ƒå’Œå›ç­”ï¼Œä»è€Œå®ç°å¤–éƒ¨å‡½æ•°çš„è°ƒç”¨è¿‡ç¨‹ï¼Œå³ï¼š
```json
# å°†ä¸€ä¸ªæ’ä»¶çš„å…³é”®ä¿¡æ¯æ‹¼æ¥æˆä¸€æ®µæ–‡æœ¬çš„æ¨¡ç‰ˆã€‚
TOOL_DESC = """{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}"""

# ReAct prompting çš„ instruction æ¨¡ç‰ˆï¼Œå°†åŒ…å«æ’ä»¶çš„è¯¦ç»†ä¿¡æ¯ã€‚
PROMPT_REACT = """Answer the following questions as best you can. You have access to the following APIs:

{tool_descs}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can be repeated zero or more times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {query}"""
```

&emsp;&emsp;å› æ­¤ï¼Œå¼•å…¥Prompt Templateå¯ä»¥æ”¯æŒå˜é‡å’ŒåŠ¨æ€å†…å®¹çš„æ’å…¥ï¼Œä½¿å¾—åŒä¸€ä¸ªåº”ç”¨å¯ä»¥æ ¹æ®ä¸åŒçš„è¾“å…¥åŠ¨æ€è°ƒæ•´æç¤ºè¯ï¼Œä»è€Œæ›´å¥½åœ°å“åº”ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ã€‚LangChainé€šè¿‡è¿™ç§æ–¹å¼æ¥æé«˜åº”ç”¨çš„é€šç”¨æ€§å’Œç”¨æˆ·ä½“éªŒã€‚

- **Predict**

&emsp;&emsp;åœ¨Predictéƒ¨åˆ†ï¼Œå®è´¨ä¸Šæ˜¯å¤„ç†æ¨¡å‹ä»æ¥æ”¶è¾“å…¥åˆ°æ‰§è¡Œæ¨ç†çš„æ•´ä¸ªè¿‡ç¨‹ã€‚è€ƒè™‘åˆ°å­˜åœ¨ä¸¤ç§ä¸»è¦ç±»å‹çš„å¤§æ¨¡å‹â€”â€”Baseç±»æ¨¡å‹å’ŒChatç±»æ¨¡å‹ï¼ŒLangChainåœ¨å…¶Model I/Oæ¨¡å—ä¸­å¯¹è¿™ä¸¤ç§æ¨¡å‹éƒ½è¿›è¡Œäº†æŠ½è±¡ï¼Œåˆ†åˆ«å½’ç±»ä¸ºLLMsï¼ˆLarge Language Modelsï¼‰å’ŒChat Modelsã€‚æˆ‘ä»¬è¿˜æ˜¯ä»¥OpenAI çš„ Completion å’Œ Chatcompletionsæ–¹æ³•ä¸ºä¾‹ï¼š

```python

# Baseç±»æ¨¡å‹
client.completions.create(
  model="gpt-3.5-turbo-instruct",
  prompt="Say this is a test",
)


# èŠå¤©æ¨¡å‹
client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„AIæ™ºèƒ½å°åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
  ]
)
```

&emsp;&emsp;LLMsæ˜¯ç®€åŒ–çš„å¤§è¯­è¨€æ¨¡å‹æŠ½è±¡ï¼Œå³åŸºäºç»™å®šçš„Promptæä¾›å†…å®¹ç”Ÿæˆçš„åŠŸèƒ½ã€‚è€ŒChat Modelsåˆ™ä¸“æ³¨äºèŠå¤©APIçš„æŠ½è±¡ï¼Œéœ€è¦ç»´æŠ¤ä¸Šä¸‹æ–‡çš„è®°å¿†ï¼ˆèŠå¤©è®°å½•ï¼‰ï¼Œå‘ˆç°å‡ºæ›´æ¥è¿‘å¯¹è¯æˆ–èŠå¤©å½¢å¼çš„äº¤äº’ã€‚

- **Parse**

&emsp;&emsp;æˆ‘ä»¬çŸ¥é“ï¼Œå¤§æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸ç¨³å®šçš„ï¼ŒåŒæ ·çš„è¾“å…¥Promptå¾€å¾€ä¼šå¾—åˆ°ä¸åŒå½¢å¼çš„è¾“å‡ºã€‚åœ¨è‡ªç„¶è¯­è¨€äº¤äº’ä¸­ï¼Œä¸åŒçš„è¯­è¨€è¡¨è¾¾æ–¹å¼é€šå¸¸ä¸ä¼šé€ æˆç†è§£ä¸Šçš„éšœç¢ã€‚ä½†åœ¨åº”ç”¨å¼€å‘ä¸­ï¼Œå¤§æ¨¡å‹çš„è¾“å‡ºå¯èƒ½æ˜¯ä¸‹ä¸€æ­¥é€»è¾‘å¤„ç†çš„å…³é”®è¾“å…¥ã€‚å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè§„èŒƒåŒ–è¾“å‡ºæ˜¯å¿…é¡»è¦åšçš„ä»»åŠ¡ï¼Œä»¥ç¡®ä¿åº”ç”¨èƒ½å¤Ÿé¡ºåˆ©è¿›è¡Œåç»­çš„é€»è¾‘å¤„ç†ã€‚

&emsp;&emsp;è¾“å‡ºè§£æå™¨ Output Parserå°±æ˜¯ä¸€ä¸ªå¸®åŠ©ç»“æ„åŒ–è¯­è¨€æ¨¡å‹å“åº”çš„æŠ½è±¡ï¼Œå¯ä»¥è·å–æ ¼å¼æŒ‡ä»¤æˆ–è€…è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„è§£æã€‚è¿™æˆ‘ä»¬ä¼šåœ¨åé¢çš„å®è·µä¸­ç›´è§‚çš„ä½“éªŒåˆ°ã€‚

&emsp;&emsp;æ•´ä½“è€Œè¨€ï¼Œåœ¨Model I/Oæ¨¡å—çš„æŠ½è±¡ä¸­ï¼Œå…¶ä¸€èƒ½å¤Ÿè®©å¼€å‘è€…å¿«é€Ÿçš„æ¥å…¥ä¸åŒçš„å¤§æ¨¡å‹ï¼Œæ¯”å¦‚OpenAIã€ChatGLMã€Qwenç­‰ï¼ŒæŒ‰ç…§æ—¢å®šè§„èŒƒæ‰§è¡Œæ¨¡å‹æ¨ç†ã€‚å…¶äºŒé€šè¿‡è¾“å…¥å’Œè¾“å‡ºçš„æ¨¡æ¿åŒ–å¤„ç†ï¼Œä½¿å…¶æ›´è´´åˆäºåº”ç”¨å¼€å‘çš„æœ€ä½³å®è·µã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±é€æ­¥çš„ä»‹ç»ä¸Šè¿°ä¸‰ä¸ªæµç¨‹åœ¨LangChainä¸‹æ˜¯å¦‚ä½•è¿›è¡Œé›†æˆå’Œæ“ä½œçš„ã€‚

<div align=center><img src="https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202501131756947.png" width=100%></div>

# 2. DeepSeek v3 æ¨¡å‹æ³¨å†Œä¸ä½¿ç”¨

- DeepSeek v3è´¦å·æ³¨å†Œä¸APIè·å–

&emsp;&emsp;`DeepSeek`å®˜ç½‘ï¼šhttps://www.deepseek.com/

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107155848673.png" alt="image-20250107155848673" style="zoom:33%;" />

&emsp;&emsp;æ–°ç”¨æˆ·æ³¨å†Œå³èµ é€10å…ƒé¢åº¦ï¼Œçº¦500ä¸‡tokené¢åº¦ï¼ˆå¦‚æœå‘ç°æ²¡æœ‰èµ é€é¢åº¦ï¼Œé‚£å°±æ˜¯å®˜æ–¹åœæ­¢äº†æ´»åŠ¨ã€‚ï¼‰

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107155925049.png" alt="image-20250107155925049" style="zoom:25%;" />

&emsp;&emsp;å¯¹æ¯”`GPT4o`ä»·æ ¼ï¼Œçº¦é™ä½`90%`ä»¥ä¸Šï¼šè¾“å…¥ä»·æ ¼ä¸º`GPT4o`çš„`6%`ï¼Œè¾“å‡ºä»·æ ¼ä¸º`GPT4o`çš„`3%`ã€‚

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107160101536.png" alt="image-20250107160101536" style="zoom:33%;" />

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107160418665.png" alt="image-20250107160418665" style="zoom: 50%;" />

&emsp;&emsp;è€Œä¸”å…¶`API`è°ƒç”¨ä¸é™é€Ÿï¼š

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107160659366.png" alt="image-20250107160659366" style="zoom:33%;" />

&emsp;&emsp;æœ€å…³é”®çš„æ˜¯ï¼Œè°ƒç”¨é£æ ¼å’Œ`OpenAI`å®Œå…¨ä¸€è‡´ï¼š`Function calling`ã€`Json Output`ç­‰åŠŸèƒ½å®Œå…¨ç›¸åŒï¼š

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107161014344.png" alt="image-20250107161014344" style="zoom:33%;" />

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107161058708.png" alt="image-20250107161058708" style="zoom:33%;" />

# 3. åŸºäºLCEL å®ç° DeepSeep v3 çš„é›†æˆ

&emsp;&emsp;`LangChain`ä¸­æœ€åŸºæœ¬å’Œå¸¸è§çš„ç”¨ä¾‹æ˜¯å°†æç¤ºæ¨¡æ¿å’Œæ¨¡å‹é“¾æ¥åœ¨ä¸€èµ·ã€‚ä¸ºäº†çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¥å—æŸä¸ªä¸»é¢˜å¹¶ç”Ÿæˆå“åº”çš„é“¾ï¼š


```python
# ! pip install -qU langchain langchain-openai
```


```python
from langchain_openai import ChatOpenAI

# ä½¿ç”¨DeepSeek-chatæ¨¡å‹ä½œä¸ºchatmodel
model = ChatOpenAI(model="deepseek-chat",
                   api_key='sk-6c960cb2a7',
                   base_url='https://api.deepseek.com')
```

&emsp;&emsp;ä½¿ç”¨ `LCEL` å°†è¿™äº›ä¸åŒçš„ç»„ä»¶æ‹¼å‡‘æˆä¸€ä¸ªé“¾ã€‚å…¶ä¸­ `|` ç¬¦å·ç±»ä¼¼äº`unix`ç®¡é“è¿ç®—ç¬¦ï¼Œå®ƒå°†ä¸åŒçš„ç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·ï¼Œå°†ä¸€ä¸ªç»„ä»¶çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªç»„ä»¶çš„è¾“å…¥ã€‚


```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("è¯·ä½ ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯ {topic}")

chain = prompt | model | StrOutputParser()
```

&emsp;&emsp;åœ¨æ­¤é“¾ä¸­ï¼Œç”¨æˆ·è¾“å…¥ä¼ é€’åˆ°æç¤ºæ¨¡æ¿ï¼Œç„¶åæç¤ºæ¨¡æ¿è¾“å‡ºä¼ é€’åˆ°æ¨¡å‹ï¼Œç„¶åæ¨¡å‹è¾“å‡ºä¼ é€’åˆ°è¾“å‡ºè§£æå™¨ã€‚


```python
chain.invoke({"topic": "äººå·¥æ™ºèƒ½"})
```




    'äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§° AIï¼‰æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç³»ç»Ÿæ¨¡æ‹Ÿã€æ‰©å±•å’Œå¢å¼ºäººç±»æ™ºèƒ½çš„æŠ€æœ¯å’Œç§‘å­¦é¢†åŸŸã€‚å®ƒæ—¨åœ¨ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’Œå†³ç­–ç­‰ã€‚\n\n### äººå·¥æ™ºèƒ½çš„ä¸»è¦ç‰¹ç‚¹ï¼š\n1. **æ¨¡æ‹Ÿäººç±»æ™ºèƒ½**ï¼šAI ç³»ç»Ÿè¯•å›¾æ¨¡ä»¿äººç±»çš„æ€ç»´è¿‡ç¨‹ï¼Œä¾‹å¦‚å­¦ä¹ ã€æ¨ç†å’Œå†³ç­–ã€‚\n2. **è‡ªåŠ¨åŒ–**ï¼šAI å¯ä»¥è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡ï¼Œå‡å°‘äººç±»å¹²é¢„ã€‚\n3. **é€‚åº”æ€§å’Œå­¦ä¹ èƒ½åŠ›**ï¼šè®¸å¤š AI ç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡æ•°æ®å­¦ä¹ å¹¶æ”¹è¿›æ€§èƒ½ï¼Œä¾‹å¦‚æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ã€‚\n4. **å¤„ç†å¤æ‚é—®é¢˜**ï¼šAI å¯ä»¥å¤„ç†å¤§é‡æ•°æ®å¹¶è§£å†³å¤æ‚é—®é¢˜ï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œé¢„æµ‹åˆ†æã€‚\n\n### äººå·¥æ™ºèƒ½çš„åˆ†ç±»ï¼š\n1. **å¼±äººå·¥æ™ºèƒ½ï¼ˆNarrow AIï¼‰**ï¼šä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œä¾‹å¦‚è¯­éŸ³åŠ©æ‰‹ï¼ˆå¦‚ Siriã€Alexaï¼‰æˆ–å›¾åƒè¯†åˆ«ç³»ç»Ÿã€‚è¿™æ˜¯ç›®å‰æœ€å¸¸è§çš„ AI å½¢å¼ã€‚\n2. **å¼ºäººå·¥æ™ºèƒ½ï¼ˆGeneral AIï¼‰**ï¼šå…·å¤‡ä¸äººç±»ç›¸å½“çš„é€šç”¨æ™ºèƒ½ï¼Œèƒ½å¤Ÿç†è§£ã€å­¦ä¹ å’Œæ‰§è¡Œä»»ä½•æ™ºåŠ›ä»»åŠ¡ã€‚ç›®å‰å°šæœªå®ç°ã€‚\n3. **è¶…çº§äººå·¥æ™ºèƒ½ï¼ˆSuperintelligent AIï¼‰**ï¼šè¶…è¶Šäººç±»æ™ºèƒ½çš„ AIï¼Œèƒ½å¤Ÿåœ¨å‡ ä¹æ‰€æœ‰é¢†åŸŸè¶…è¶Šäººç±»ã€‚è¿™ä»ç„¶æ˜¯ç†è®ºä¸Šçš„æ¦‚å¿µã€‚\n\n### äººå·¥æ™ºèƒ½çš„å…³é”®æŠ€æœ¯ï¼š\n1. **æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰**ï¼šé€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ å¹¶æ”¹è¿›æ€§èƒ½ã€‚\n2. **æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰**ï¼šä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ï¼Œé€‚ç”¨äºå›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ã€‚\n3. **è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰**ï¼šä½¿æœºå™¨èƒ½å¤Ÿç†è§£ã€ç”Ÿæˆå’Œå¤„ç†äººç±»è¯­è¨€ï¼Œä¾‹å¦‚èŠå¤©æœºå™¨äººå’Œç¿»è¯‘ç³»ç»Ÿã€‚\n4. **è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰**ï¼šä½¿æœºå™¨èƒ½å¤Ÿâ€œçœ‹â€å¹¶ç†è§£å›¾åƒå’Œè§†é¢‘å†…å®¹ï¼Œä¾‹å¦‚äººè„¸è¯†åˆ«å’Œè‡ªåŠ¨é©¾é©¶ã€‚\n5. **å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰**ï¼šé€šè¿‡è¯•é”™å’Œå¥–åŠ±æœºåˆ¶è®­ç»ƒ AI ç³»ç»Ÿï¼Œä½¿å…¶åœ¨ç‰¹å®šç¯å¢ƒä¸­åšå‡ºæœ€ä½³å†³ç­–ã€‚\n\n### äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸï¼š\n- **åŒ»ç–—**ï¼šç–¾ç—…è¯Šæ–­ã€è¯ç‰©ç ”å‘ã€ä¸ªæ€§åŒ–æ²»ç–—ã€‚\n- **é‡‘è**ï¼šé£é™©è¯„ä¼°ã€æ¬ºè¯ˆæ£€æµ‹ã€è‡ªåŠ¨åŒ–äº¤æ˜“ã€‚\n- **äº¤é€š**ï¼šè‡ªåŠ¨é©¾é©¶ã€äº¤é€šæµé‡ä¼˜åŒ–ã€‚\n- **æ•™è‚²**ï¼šä¸ªæ€§åŒ–å­¦ä¹ ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿã€‚\n- **å¨±ä¹**ï¼šæ¸¸æˆ AIã€å†…å®¹æ¨èç³»ç»Ÿã€‚\n- **åˆ¶é€ ä¸š**ï¼šè‡ªåŠ¨åŒ–ç”Ÿäº§çº¿ã€è´¨é‡æ§åˆ¶ã€‚\n\n### äººå·¥æ™ºèƒ½çš„æŒ‘æˆ˜ä¸äº‰è®®ï¼š\n1. **ä¼¦ç†é—®é¢˜**ï¼šAI çš„å†³ç­–å¯èƒ½æ¶‰åŠéšç§ã€åè§å’Œå…¬å¹³æ€§ç­‰é—®é¢˜ã€‚\n2. **å°±ä¸šå½±å“**ï¼šè‡ªåŠ¨åŒ–å¯èƒ½å–ä»£æŸäº›å·¥ä½œå²—ä½ï¼Œå¼•å‘ç¤¾ä¼šç»æµé—®é¢˜ã€‚\n3. **å®‰å…¨æ€§**ï¼šAI ç³»ç»Ÿå¯èƒ½è¢«æ»¥ç”¨æˆ–å‡ºç°ä¸å¯é¢„æµ‹çš„è¡Œä¸ºã€‚\n4. **æŠ€æœ¯é™åˆ¶**ï¼šç›®å‰çš„ AI ä»æ— æ³•å®Œå…¨æ¨¡æ‹Ÿäººç±»çš„åˆ›é€ åŠ›å’Œæƒ…æ„Ÿã€‚\n\næ€»ä¹‹ï¼Œäººå·¥æ™ºèƒ½æ˜¯ä¸€é¡¹å¿«é€Ÿå‘å±•çš„æŠ€æœ¯ï¼Œæ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚å°½ç®¡å®ƒå¸¦æ¥äº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä¹Ÿéœ€è¦è°¨æ…åº”å¯¹å…¶å¸¦æ¥çš„æŒ‘æˆ˜ã€‚'



&emsp;&emsp;åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œ`Runnable` ä¼šåŠ¨æ€ç”Ÿæˆè¾“å…¥çš„æè¿°ï¼Œæ˜¾ç¤ºçš„å°†å…¶ä¼ å…¥`Pydantic` æ¨¡å‹ã€‚æˆ‘ä»¬å¯ä»¥å¯¹å…¶è°ƒç”¨.schema()æ¥è·å– JSONSchema è¡¨ç¤ºã€‚


```python
chain.input_schema.schema()
```




    {'properties': {'topic': {'title': 'Topic', 'type': 'string'}},
     'required': ['topic'],
     'title': 'PromptInput',
     'type': 'object'}




```python
prompt.input_schema.schema()
```




    {'properties': {'topic': {'title': 'Topic', 'type': 'string'}},
     'required': ['topic'],
     'title': 'PromptInput',
     'type': 'object'}




```python
prompt.output_schema.schema()
```




    {'$defs': {'AIMessage': {'additionalProperties': True,
       'description': 'Message from an AI.\n\nAIMessage is returned from a chat model as a response to a prompt.\n\nThis message represents the output of the model and consists of both\nthe raw output as returned by the model together standardized fields\n(e.g., tool calls, usage metadata) added by the LangChain framework.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'ai',
         'default': 'ai',
         'enum': ['ai'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'},
        'tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/ToolCall'},
         'title': 'Tool Calls',
         'type': 'array'},
        'invalid_tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/InvalidToolCall'},
         'title': 'Invalid Tool Calls',
         'type': 'array'},
        'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},
          {'type': 'null'}],
         'default': None}},
       'required': ['content'],
       'title': 'AIMessage',
       'type': 'object'},
      'AIMessageChunk': {'additionalProperties': True,
       'description': 'Message chunk from an AI.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'AIMessageChunk',
         'default': 'AIMessageChunk',
         'enum': ['AIMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'},
        'tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/ToolCall'},
         'title': 'Tool Calls',
         'type': 'array'},
        'invalid_tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/InvalidToolCall'},
         'title': 'Invalid Tool Calls',
         'type': 'array'},
        'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},
          {'type': 'null'}],
         'default': None},
        'tool_call_chunks': {'default': [],
         'items': {'$ref': '#/$defs/ToolCallChunk'},
         'title': 'Tool Call Chunks',
         'type': 'array'}},
       'required': ['content'],
       'title': 'AIMessageChunk',
       'type': 'object'},
      'ChatMessage': {'additionalProperties': True,
       'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'chat',
         'default': 'chat',
         'enum': ['chat'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'role': {'title': 'Role', 'type': 'string'}},
       'required': ['content', 'role'],
       'title': 'ChatMessage',
       'type': 'object'},
      'ChatMessageChunk': {'additionalProperties': True,
       'description': 'Chat Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'ChatMessageChunk',
         'default': 'ChatMessageChunk',
         'enum': ['ChatMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'role': {'title': 'Role', 'type': 'string'}},
       'required': ['content', 'role'],
       'title': 'ChatMessageChunk',
       'type': 'object'},
      'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\nFor use in external schemas.',
       'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},
           {'$ref': '#/$defs/HumanMessage'},
           {'$ref': '#/$defs/ChatMessage'},
           {'$ref': '#/$defs/SystemMessage'},
           {'$ref': '#/$defs/FunctionMessage'},
           {'$ref': '#/$defs/ToolMessage'},
           {'$ref': '#/$defs/AIMessageChunk'},
           {'$ref': '#/$defs/HumanMessageChunk'},
           {'$ref': '#/$defs/ChatMessageChunk'},
           {'$ref': '#/$defs/SystemMessageChunk'},
           {'$ref': '#/$defs/FunctionMessageChunk'},
           {'$ref': '#/$defs/ToolMessageChunk'}]},
         'title': 'Messages',
         'type': 'array'},
        'type': {'const': 'ChatPromptValueConcrete',
         'default': 'ChatPromptValueConcrete',
         'enum': ['ChatPromptValueConcrete'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['messages'],
       'title': 'ChatPromptValueConcrete',
       'type': 'object'},
      'FunctionMessage': {'additionalProperties': True,
       'description': 'Message for passing the result of executing a tool back to a model.\n\nFunctionMessage are an older version of the ToolMessage schema, and\ndo not contain the tool_call_id field.\n\nThe tool_call_id field is used to associate the tool call request with the\ntool call response. This is useful in situations where a chat model is able\nto request multiple tool calls in parallel.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'function',
         'default': 'function',
         'enum': ['function'],
         'title': 'Type',
         'type': 'string'},
        'name': {'title': 'Name', 'type': 'string'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content', 'name'],
       'title': 'FunctionMessage',
       'type': 'object'},
      'FunctionMessageChunk': {'additionalProperties': True,
       'description': 'Function Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'FunctionMessageChunk',
         'default': 'FunctionMessageChunk',
         'enum': ['FunctionMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'title': 'Name', 'type': 'string'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content', 'name'],
       'title': 'FunctionMessageChunk',
       'type': 'object'},
      'HumanMessage': {'additionalProperties': True,
       'description': 'Message from a human.\n\nHumanMessages are messages that are passed in from a human to the model.\n\nExample:\n\n    .. code-block:: python\n\n        from langchain_core.messages import HumanMessage, SystemMessage\n\n        messages = [\n            SystemMessage(\n                content="You are a helpful assistant! Your name is Bob."\n            ),\n            HumanMessage(\n                content="What is your name?"\n            )\n        ]\n\n        # Instantiate a chat model and invoke it with the messages\n        model = ...\n        print(model.invoke(messages))',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'human',
         'default': 'human',
         'enum': ['human'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},
       'required': ['content'],
       'title': 'HumanMessage',
       'type': 'object'},
      'HumanMessageChunk': {'additionalProperties': True,
       'description': 'Human Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'HumanMessageChunk',
         'default': 'HumanMessageChunk',
         'enum': ['HumanMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},
       'required': ['content'],
       'title': 'HumanMessageChunk',
       'type': 'object'},
      'InputTokenDetails': {'description': 'Breakdown of input token counts.\n\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "audio": 10,\n            "cache_creation": 200,\n            "cache_read": 100,\n        }\n\n.. versionadded:: 0.3.9',
       'properties': {'audio': {'title': 'Audio', 'type': 'integer'},
        'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},
        'cache_read': {'title': 'Cache Read', 'type': 'integer'}},
       'title': 'InputTokenDetails',
       'type': 'object'},
      'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\n\nHere we add an `error` key to surface errors made during generation\n(e.g., invalid JSON arguments.)',
       'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'title': 'Name'},
        'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},
        'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'title': 'Error'},
        'type': {'const': 'invalid_tool_call',
         'enum': ['invalid_tool_call'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['name', 'args', 'id', 'error'],
       'title': 'InvalidToolCall',
       'type': 'object'},
      'OutputTokenDetails': {'description': 'Breakdown of output token counts.\n\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "audio": 10,\n            "reasoning": 200,\n        }\n\n.. versionadded:: 0.3.9',
       'properties': {'audio': {'title': 'Audio', 'type': 'integer'},
        'reasoning': {'title': 'Reasoning', 'type': 'integer'}},
       'title': 'OutputTokenDetails',
       'type': 'object'},
      'StringPromptValue': {'description': 'String prompt value.',
       'properties': {'text': {'title': 'Text', 'type': 'string'},
        'type': {'const': 'StringPromptValue',
         'default': 'StringPromptValue',
         'enum': ['StringPromptValue'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['text'],
       'title': 'StringPromptValue',
       'type': 'object'},
      'SystemMessage': {'additionalProperties': True,
       'description': 'Message for priming AI behavior.\n\nThe system message is usually passed in as the first of a sequence\nof input messages.\n\nExample:\n\n    .. code-block:: python\n\n        from langchain_core.messages import HumanMessage, SystemMessage\n\n        messages = [\n            SystemMessage(\n                content="You are a helpful assistant! Your name is Bob."\n            ),\n            HumanMessage(\n                content="What is your name?"\n            )\n        ]\n\n        # Define a chat model and invoke it with the messages\n        print(model.invoke(messages))',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'system',
         'default': 'system',
         'enum': ['system'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content'],
       'title': 'SystemMessage',
       'type': 'object'},
      'SystemMessageChunk': {'additionalProperties': True,
       'description': 'System Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'SystemMessageChunk',
         'default': 'SystemMessageChunk',
         'enum': ['SystemMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content'],
       'title': 'SystemMessageChunk',
       'type': 'object'},
      'ToolCall': {'description': 'Represents a request to call a tool.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "name": "foo",\n            "args": {"a": 1},\n            "id": "123"\n        }\n\n    This represents a request to call the tool named "foo" with arguments {"a": 1}\n    and an identifier of "123".',
       'properties': {'name': {'title': 'Name', 'type': 'string'},
        'args': {'title': 'Args', 'type': 'object'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},
        'type': {'const': 'tool_call',
         'enum': ['tool_call'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['name', 'args', 'id'],
       'title': 'ToolCall',
       'type': 'object'},
      'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\n\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\nall string attributes are concatenated. Chunks are only merged if their\nvalues of `index` are equal and not None.\n\nExample:\n\n.. code-block:: python\n\n    left_chunks = [ToolCallChunk(name="foo", args=\'{"a":\', index=0)]\n    right_chunks = [ToolCallChunk(name=None, args=\'1}\', index=0)]\n\n    (\n        AIMessageChunk(content="", tool_call_chunks=left_chunks)\n        + AIMessageChunk(content="", tool_call_chunks=right_chunks)\n    ).tool_call_chunks == [ToolCallChunk(name=\'foo\', args=\'{"a":1}\', index=0)]',
       'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'title': 'Name'},
        'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},
        'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],
         'title': 'Index'},
        'type': {'const': 'tool_call_chunk',
         'enum': ['tool_call_chunk'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['name', 'args', 'id', 'index'],
       'title': 'ToolCallChunk',
       'type': 'object'},
      'ToolMessage': {'additionalProperties': True,
       'description': 'Message for passing the result of executing a tool back to a model.\n\nToolMessages contain the result of a tool invocation. Typically, the result\nis encoded inside the `content` field.\n\nExample: A ToolMessage representing a result of 42 from a tool call with id\n\n    .. code-block:: python\n\n        from langchain_core.messages import ToolMessage\n\n        ToolMessage(content=\'42\', tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\')\n\n\nExample: A ToolMessage where only part of the tool output is sent to the model\n    and the full output is passed in to artifact.\n\n    .. versionadded:: 0.2.17\n\n    .. code-block:: python\n\n        from langchain_core.messages import ToolMessage\n\n        tool_output = {\n            "stdout": "From the graph we can see that the correlation between x and y is ...",\n            "stderr": None,\n            "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},\n        }\n\n        ToolMessage(\n            content=tool_output["stdout"],\n            artifact=tool_output,\n            tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\',\n        )\n\nThe tool_call_id field is used to associate the tool call request with the\ntool call response. This is useful in situations where a chat model is able\nto request multiple tool calls in parallel.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'tool',
         'default': 'tool',
         'enum': ['tool'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},
        'artifact': {'default': None, 'title': 'Artifact'},
        'status': {'default': 'success',
         'enum': ['success', 'error'],
         'title': 'Status',
         'type': 'string'}},
       'required': ['content', 'tool_call_id'],
       'title': 'ToolMessage',
       'type': 'object'},
      'ToolMessageChunk': {'additionalProperties': True,
       'description': 'Tool Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'ToolMessageChunk',
         'default': 'ToolMessageChunk',
         'enum': ['ToolMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},
        'artifact': {'default': None, 'title': 'Artifact'},
        'status': {'default': 'success',
         'enum': ['success', 'error'],
         'title': 'Status',
         'type': 'string'}},
       'required': ['content', 'tool_call_id'],
       'title': 'ToolMessageChunk',
       'type': 'object'},
      'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\n\nThis is a standard representation of token usage that is consistent across models.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "input_tokens": 350,\n            "output_tokens": 240,\n            "total_tokens": 590,\n            "input_token_details": {\n                "audio": 10,\n                "cache_creation": 200,\n                "cache_read": 100,\n            },\n            "output_token_details": {\n                "audio": 10,\n                "reasoning": 200,\n            }\n        }\n\n.. versionchanged:: 0.3.9\n\n    Added ``input_token_details`` and ``output_token_details``.',
       'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},
        'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},
        'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},
        'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},
        'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},
       'required': ['input_tokens', 'output_tokens', 'total_tokens'],
       'title': 'UsageMetadata',
       'type': 'object'}},
     'anyOf': [{'$ref': '#/$defs/StringPromptValue'},
      {'$ref': '#/$defs/ChatPromptValueConcrete'}],
     'title': 'ChatPromptTemplateOutput'}




```python
model.input_schema.schema()
```




    {'$defs': {'AIMessage': {'additionalProperties': True,
       'description': 'Message from an AI.\n\nAIMessage is returned from a chat model as a response to a prompt.\n\nThis message represents the output of the model and consists of both\nthe raw output as returned by the model together standardized fields\n(e.g., tool calls, usage metadata) added by the LangChain framework.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'ai',
         'default': 'ai',
         'enum': ['ai'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'},
        'tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/ToolCall'},
         'title': 'Tool Calls',
         'type': 'array'},
        'invalid_tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/InvalidToolCall'},
         'title': 'Invalid Tool Calls',
         'type': 'array'},
        'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},
          {'type': 'null'}],
         'default': None}},
       'required': ['content'],
       'title': 'AIMessage',
       'type': 'object'},
      'AIMessageChunk': {'additionalProperties': True,
       'description': 'Message chunk from an AI.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'AIMessageChunk',
         'default': 'AIMessageChunk',
         'enum': ['AIMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'},
        'tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/ToolCall'},
         'title': 'Tool Calls',
         'type': 'array'},
        'invalid_tool_calls': {'default': [],
         'items': {'$ref': '#/$defs/InvalidToolCall'},
         'title': 'Invalid Tool Calls',
         'type': 'array'},
        'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},
          {'type': 'null'}],
         'default': None},
        'tool_call_chunks': {'default': [],
         'items': {'$ref': '#/$defs/ToolCallChunk'},
         'title': 'Tool Call Chunks',
         'type': 'array'}},
       'required': ['content'],
       'title': 'AIMessageChunk',
       'type': 'object'},
      'ChatMessage': {'additionalProperties': True,
       'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'chat',
         'default': 'chat',
         'enum': ['chat'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'role': {'title': 'Role', 'type': 'string'}},
       'required': ['content', 'role'],
       'title': 'ChatMessage',
       'type': 'object'},
      'ChatMessageChunk': {'additionalProperties': True,
       'description': 'Chat Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'ChatMessageChunk',
         'default': 'ChatMessageChunk',
         'enum': ['ChatMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'role': {'title': 'Role', 'type': 'string'}},
       'required': ['content', 'role'],
       'title': 'ChatMessageChunk',
       'type': 'object'},
      'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\nFor use in external schemas.',
       'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},
           {'$ref': '#/$defs/HumanMessage'},
           {'$ref': '#/$defs/ChatMessage'},
           {'$ref': '#/$defs/SystemMessage'},
           {'$ref': '#/$defs/FunctionMessage'},
           {'$ref': '#/$defs/ToolMessage'},
           {'$ref': '#/$defs/AIMessageChunk'},
           {'$ref': '#/$defs/HumanMessageChunk'},
           {'$ref': '#/$defs/ChatMessageChunk'},
           {'$ref': '#/$defs/SystemMessageChunk'},
           {'$ref': '#/$defs/FunctionMessageChunk'},
           {'$ref': '#/$defs/ToolMessageChunk'}]},
         'title': 'Messages',
         'type': 'array'},
        'type': {'const': 'ChatPromptValueConcrete',
         'default': 'ChatPromptValueConcrete',
         'enum': ['ChatPromptValueConcrete'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['messages'],
       'title': 'ChatPromptValueConcrete',
       'type': 'object'},
      'FunctionMessage': {'additionalProperties': True,
       'description': 'Message for passing the result of executing a tool back to a model.\n\nFunctionMessage are an older version of the ToolMessage schema, and\ndo not contain the tool_call_id field.\n\nThe tool_call_id field is used to associate the tool call request with the\ntool call response. This is useful in situations where a chat model is able\nto request multiple tool calls in parallel.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'function',
         'default': 'function',
         'enum': ['function'],
         'title': 'Type',
         'type': 'string'},
        'name': {'title': 'Name', 'type': 'string'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content', 'name'],
       'title': 'FunctionMessage',
       'type': 'object'},
      'FunctionMessageChunk': {'additionalProperties': True,
       'description': 'Function Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'FunctionMessageChunk',
         'default': 'FunctionMessageChunk',
         'enum': ['FunctionMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'title': 'Name', 'type': 'string'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content', 'name'],
       'title': 'FunctionMessageChunk',
       'type': 'object'},
      'HumanMessage': {'additionalProperties': True,
       'description': 'Message from a human.\n\nHumanMessages are messages that are passed in from a human to the model.\n\nExample:\n\n    .. code-block:: python\n\n        from langchain_core.messages import HumanMessage, SystemMessage\n\n        messages = [\n            SystemMessage(\n                content="You are a helpful assistant! Your name is Bob."\n            ),\n            HumanMessage(\n                content="What is your name?"\n            )\n        ]\n\n        # Instantiate a chat model and invoke it with the messages\n        model = ...\n        print(model.invoke(messages))',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'human',
         'default': 'human',
         'enum': ['human'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},
       'required': ['content'],
       'title': 'HumanMessage',
       'type': 'object'},
      'HumanMessageChunk': {'additionalProperties': True,
       'description': 'Human Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'HumanMessageChunk',
         'default': 'HumanMessageChunk',
         'enum': ['HumanMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},
       'required': ['content'],
       'title': 'HumanMessageChunk',
       'type': 'object'},
      'InputTokenDetails': {'description': 'Breakdown of input token counts.\n\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "audio": 10,\n            "cache_creation": 200,\n            "cache_read": 100,\n        }\n\n.. versionadded:: 0.3.9',
       'properties': {'audio': {'title': 'Audio', 'type': 'integer'},
        'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},
        'cache_read': {'title': 'Cache Read', 'type': 'integer'}},
       'title': 'InputTokenDetails',
       'type': 'object'},
      'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\n\nHere we add an `error` key to surface errors made during generation\n(e.g., invalid JSON arguments.)',
       'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'title': 'Name'},
        'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},
        'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'title': 'Error'},
        'type': {'const': 'invalid_tool_call',
         'enum': ['invalid_tool_call'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['name', 'args', 'id', 'error'],
       'title': 'InvalidToolCall',
       'type': 'object'},
      'OutputTokenDetails': {'description': 'Breakdown of output token counts.\n\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "audio": 10,\n            "reasoning": 200,\n        }\n\n.. versionadded:: 0.3.9',
       'properties': {'audio': {'title': 'Audio', 'type': 'integer'},
        'reasoning': {'title': 'Reasoning', 'type': 'integer'}},
       'title': 'OutputTokenDetails',
       'type': 'object'},
      'StringPromptValue': {'description': 'String prompt value.',
       'properties': {'text': {'title': 'Text', 'type': 'string'},
        'type': {'const': 'StringPromptValue',
         'default': 'StringPromptValue',
         'enum': ['StringPromptValue'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['text'],
       'title': 'StringPromptValue',
       'type': 'object'},
      'SystemMessage': {'additionalProperties': True,
       'description': 'Message for priming AI behavior.\n\nThe system message is usually passed in as the first of a sequence\nof input messages.\n\nExample:\n\n    .. code-block:: python\n\n        from langchain_core.messages import HumanMessage, SystemMessage\n\n        messages = [\n            SystemMessage(\n                content="You are a helpful assistant! Your name is Bob."\n            ),\n            HumanMessage(\n                content="What is your name?"\n            )\n        ]\n\n        # Define a chat model and invoke it with the messages\n        print(model.invoke(messages))',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'system',
         'default': 'system',
         'enum': ['system'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content'],
       'title': 'SystemMessage',
       'type': 'object'},
      'SystemMessageChunk': {'additionalProperties': True,
       'description': 'System Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'SystemMessageChunk',
         'default': 'SystemMessageChunk',
         'enum': ['SystemMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'}},
       'required': ['content'],
       'title': 'SystemMessageChunk',
       'type': 'object'},
      'ToolCall': {'description': 'Represents a request to call a tool.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "name": "foo",\n            "args": {"a": 1},\n            "id": "123"\n        }\n\n    This represents a request to call the tool named "foo" with arguments {"a": 1}\n    and an identifier of "123".',
       'properties': {'name': {'title': 'Name', 'type': 'string'},
        'args': {'title': 'Args', 'type': 'object'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},
        'type': {'const': 'tool_call',
         'enum': ['tool_call'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['name', 'args', 'id'],
       'title': 'ToolCall',
       'type': 'object'},
      'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\n\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\nall string attributes are concatenated. Chunks are only merged if their\nvalues of `index` are equal and not None.\n\nExample:\n\n.. code-block:: python\n\n    left_chunks = [ToolCallChunk(name="foo", args=\'{"a":\', index=0)]\n    right_chunks = [ToolCallChunk(name=None, args=\'1}\', index=0)]\n\n    (\n        AIMessageChunk(content="", tool_call_chunks=left_chunks)\n        + AIMessageChunk(content="", tool_call_chunks=right_chunks)\n    ).tool_call_chunks == [ToolCallChunk(name=\'foo\', args=\'{"a":1}\', index=0)]',
       'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'title': 'Name'},
        'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},
        'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],
         'title': 'Index'},
        'type': {'const': 'tool_call_chunk',
         'enum': ['tool_call_chunk'],
         'title': 'Type',
         'type': 'string'}},
       'required': ['name', 'args', 'id', 'index'],
       'title': 'ToolCallChunk',
       'type': 'object'},
      'ToolMessage': {'additionalProperties': True,
       'description': 'Message for passing the result of executing a tool back to a model.\n\nToolMessages contain the result of a tool invocation. Typically, the result\nis encoded inside the `content` field.\n\nExample: A ToolMessage representing a result of 42 from a tool call with id\n\n    .. code-block:: python\n\n        from langchain_core.messages import ToolMessage\n\n        ToolMessage(content=\'42\', tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\')\n\n\nExample: A ToolMessage where only part of the tool output is sent to the model\n    and the full output is passed in to artifact.\n\n    .. versionadded:: 0.2.17\n\n    .. code-block:: python\n\n        from langchain_core.messages import ToolMessage\n\n        tool_output = {\n            "stdout": "From the graph we can see that the correlation between x and y is ...",\n            "stderr": None,\n            "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},\n        }\n\n        ToolMessage(\n            content=tool_output["stdout"],\n            artifact=tool_output,\n            tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\',\n        )\n\nThe tool_call_id field is used to associate the tool call request with the\ntool call response. This is useful in situations where a chat model is able\nto request multiple tool calls in parallel.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'tool',
         'default': 'tool',
         'enum': ['tool'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},
        'artifact': {'default': None, 'title': 'Artifact'},
        'status': {'default': 'success',
         'enum': ['success', 'error'],
         'title': 'Status',
         'type': 'string'}},
       'required': ['content', 'tool_call_id'],
       'title': 'ToolMessage',
       'type': 'object'},
      'ToolMessageChunk': {'additionalProperties': True,
       'description': 'Tool Message chunk.',
       'properties': {'content': {'anyOf': [{'type': 'string'},
          {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},
           'type': 'array'}],
         'title': 'Content'},
        'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},
        'response_metadata': {'title': 'Response Metadata', 'type': 'object'},
        'type': {'const': 'ToolMessageChunk',
         'default': 'ToolMessageChunk',
         'enum': ['ToolMessageChunk'],
         'title': 'Type',
         'type': 'string'},
        'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Name'},
        'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],
         'default': None,
         'title': 'Id'},
        'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},
        'artifact': {'default': None, 'title': 'Artifact'},
        'status': {'default': 'success',
         'enum': ['success', 'error'],
         'title': 'Status',
         'type': 'string'}},
       'required': ['content', 'tool_call_id'],
       'title': 'ToolMessageChunk',
       'type': 'object'},
      'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\n\nThis is a standard representation of token usage that is consistent across models.\n\nExample:\n\n    .. code-block:: python\n\n        {\n            "input_tokens": 350,\n            "output_tokens": 240,\n            "total_tokens": 590,\n            "input_token_details": {\n                "audio": 10,\n                "cache_creation": 200,\n                "cache_read": 100,\n            },\n            "output_token_details": {\n                "audio": 10,\n                "reasoning": 200,\n            }\n        }\n\n.. versionchanged:: 0.3.9\n\n    Added ``input_token_details`` and ``output_token_details``.',
       'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},
        'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},
        'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},
        'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},
        'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},
       'required': ['input_tokens', 'output_tokens', 'total_tokens'],
       'title': 'UsageMetadata',
       'type': 'object'}},
     'anyOf': [{'type': 'string'},
      {'$ref': '#/$defs/StringPromptValue'},
      {'$ref': '#/$defs/ChatPromptValueConcrete'},
      {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},
         {'$ref': '#/$defs/HumanMessage'},
         {'$ref': '#/$defs/ChatMessage'},
         {'$ref': '#/$defs/SystemMessage'},
         {'$ref': '#/$defs/FunctionMessage'},
         {'$ref': '#/$defs/ToolMessage'},
         {'$ref': '#/$defs/AIMessageChunk'},
         {'$ref': '#/$defs/HumanMessageChunk'},
         {'$ref': '#/$defs/ChatMessageChunk'},
         {'$ref': '#/$defs/SystemMessageChunk'},
         {'$ref': '#/$defs/FunctionMessageChunk'},
         {'$ref': '#/$defs/ToolMessageChunk'}]},
       'type': 'array'}],
     'title': 'ChatOpenAIInput'}



- **æµå¼è¾“å‡º**


```python
for chunk in chain.stream("å¤§æ¨¡å‹æŠ€æœ¯"):
    print(chunk, end="", flush=True)
```

    å¤§æ¨¡å‹æŠ€æœ¯ï¼ˆLarge Model Technologyï¼‰æ˜¯æŒ‡åŸºäºå¤§è§„æ¨¡æ•°æ®å’Œè®¡ç®—èµ„æºè®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šå¸¸å…·æœ‰æ•°åäº¿ç”šè‡³æ•°åƒäº¿ä¸ªå‚æ•°ã€‚è¿™ç±»æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„ä»»åŠ¡å¹¶ç”Ÿæˆé«˜è´¨é‡çš„ç»“æœã€‚å¤§æ¨¡å‹æŠ€æœ¯çš„æ ¸å¿ƒåœ¨äºé€šè¿‡æµ·é‡æ•°æ®å’Œå¼ºå¤§çš„è®¡ç®—èƒ½åŠ›ï¼Œå­¦ä¹ åˆ°æ›´åŠ é€šç”¨å’Œæ·±å±‚æ¬¡çš„æ¨¡å¼ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½åŒ–çš„åº”ç”¨ã€‚
    
    ### å¤§æ¨¡å‹æŠ€æœ¯çš„ç‰¹ç‚¹
    1. **å‚æ•°è§„æ¨¡å¤§**ï¼šå¤§æ¨¡å‹çš„å‚æ•°é‡é€šå¸¸åœ¨æ•°åäº¿åˆ°æ•°åƒäº¿ä¹‹é—´ï¼Œä¾‹å¦‚ OpenAI çš„ GPT-3 æœ‰ 1750 äº¿ä¸ªå‚æ•°ã€‚
    2. **æ•°æ®éœ€æ±‚é«˜**ï¼šè®­ç»ƒå¤§æ¨¡å‹éœ€è¦æµ·é‡çš„é«˜è´¨é‡æ•°æ®ï¼Œé€šå¸¸æ¥è‡ªäº’è”ç½‘ã€ä¹¦ç±ã€è®ºæ–‡ç­‰ã€‚
    3. **è®¡ç®—èµ„æºå¯†é›†**ï¼šè®­ç»ƒå¤§æ¨¡å‹éœ€è¦é«˜æ€§èƒ½çš„è®¡ç®—è®¾å¤‡ï¼ˆå¦‚ GPUã€TPUï¼‰å’Œåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ã€‚
    4. **é€šç”¨æ€§å¼º**ï¼šå¤§æ¨¡å‹é€šå¸¸å…·æœ‰å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ã€é—®ç­”ç­‰ã€‚
    5. **è¿ç§»å­¦ä¹ èƒ½åŠ›**ï¼šå¤§æ¨¡å‹å¯ä»¥é€šè¿‡å¾®è°ƒï¼ˆFine-tuningï¼‰å¿«é€Ÿé€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œå‡å°‘å¯¹ä»»åŠ¡ç‰¹å®šæ•°æ®çš„éœ€æ±‚ã€‚
    
    ### å¤§æ¨¡å‹æŠ€æœ¯çš„åº”ç”¨
    1. **è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰**ï¼š
       - æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚ GPT ç³»åˆ—ï¼‰
       - æœºå™¨ç¿»è¯‘ï¼ˆå¦‚ Google çš„ Transformer æ¨¡å‹ï¼‰
       - é—®ç­”ç³»ç»Ÿï¼ˆå¦‚ ChatGPTï¼‰
       - æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»ç­‰
    2. **è®¡ç®—æœºè§†è§‰**ï¼š
       - å›¾åƒç”Ÿæˆï¼ˆå¦‚ DALLÂ·Eï¼‰
       - ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†ç±»
    3. **è¯­éŸ³è¯†åˆ«ä¸åˆæˆ**ï¼š
       - è¯­éŸ³è½¬æ–‡å­—ï¼ˆå¦‚ Whisperï¼‰
       - è¯­éŸ³åˆæˆï¼ˆå¦‚ Tacotronï¼‰
    4. **å¤šæ¨¡æ€ä»»åŠ¡**ï¼š
       - ç»“åˆæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ç­‰å¤šç§æ¨¡æ€çš„ä»»åŠ¡ï¼ˆå¦‚ CLIPã€Flamingoï¼‰
    
    ### å¤§æ¨¡å‹æŠ€æœ¯çš„æŒ‘æˆ˜
    1. **è®¡ç®—æˆæœ¬é«˜**ï¼šè®­ç»ƒå’Œéƒ¨ç½²å¤§æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œèƒ½æºã€‚
    2. **æ•°æ®éšç§ä¸å®‰å…¨**ï¼šå¤§æ¨¡å‹è®­ç»ƒä¾èµ–äºæµ·é‡æ•°æ®ï¼Œå¯èƒ½æ¶‰åŠéšç§æ³„éœ²é—®é¢˜ã€‚
    3. **æ¨¡å‹å¯è§£é‡Šæ€§**ï¼šå¤§æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹å¤æ‚ï¼Œéš¾ä»¥è§£é‡Šå…¶å†…éƒ¨æœºåˆ¶ã€‚
    4. **ç¯å¢ƒå½±å“**ï¼šè®­ç»ƒå¤§æ¨¡å‹æ¶ˆè€—å¤§é‡èƒ½æºï¼Œå¯èƒ½å¯¹ç¯å¢ƒé€ æˆè´Ÿé¢å½±å“ã€‚
    5. **ä¼¦ç†ä¸åè§**ï¼šå¤§æ¨¡å‹å¯èƒ½ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°åè§ï¼Œå¯¼è‡´ä¸å…¬å¹³çš„ç»“æœã€‚
    
    ### å…¸å‹çš„å¤§æ¨¡å‹
    1. **GPT ç³»åˆ—**ï¼ˆOpenAIï¼‰ï¼šå¦‚ GPT-3ã€GPT-4ï¼Œä¸“æ³¨äºæ–‡æœ¬ç”Ÿæˆå’Œè‡ªç„¶è¯­è¨€ç†è§£ã€‚
    2. **BERT**ï¼ˆGoogleï¼‰ï¼šåŸºäº Transformer æ¶æ„ï¼Œæ“…é•¿æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç­‰ä»»åŠ¡ã€‚
    3. **T5**ï¼ˆGoogleï¼‰ï¼šç»Ÿä¸€äº†å¤šç§ NLP ä»»åŠ¡çš„æ¡†æ¶ã€‚
    4. **DALLÂ·E**ï¼ˆOpenAIï¼‰ï¼šç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„å¤šæ¨¡æ€æ¨¡å‹ã€‚
    5. **PaLM**ï¼ˆGoogleï¼‰ï¼šå‚æ•°è§„æ¨¡è¾¾ 5400 äº¿çš„è¯­è¨€æ¨¡å‹ã€‚
    
    ### æœªæ¥å‘å±•æ–¹å‘
    1. **æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•**ï¼šå¦‚ç¨€ç–æ¨¡å‹ã€æ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚
    2. **å¤šæ¨¡æ€èåˆ**ï¼šç»“åˆæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ç­‰å¤šç§æ¨¡æ€çš„æ¨¡å‹ã€‚
    3. **ç»¿è‰² AI**ï¼šé™ä½å¤§æ¨¡å‹çš„èƒ½è€—å’Œç¯å¢ƒå½±å“ã€‚
    4. **ä¸ªæ€§åŒ–ä¸éšç§ä¿æŠ¤**ï¼šåœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹å®ç°ä¸ªæ€§åŒ–æœåŠ¡ã€‚
    5. **å¯è§£é‡Šæ€§ä¸å…¬å¹³æ€§**ï¼šæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œå‡å°‘åè§å’Œæ­§è§†ã€‚
    
    å¤§æ¨¡å‹æŠ€æœ¯æ­£åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œä½†ä¹Ÿé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚æœªæ¥éœ€è¦åœ¨æŠ€æœ¯ã€ä¼¦ç†å’Œç¤¾ä¼šå½±å“ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œä»¥å®ç°æ›´å¹¿æ³›çš„åº”ç”¨å’Œå¯æŒç»­å‘å±•ã€‚

&emsp;&emsp;æ•´ä¸ªè¿‡ç¨‹å…¶å®å¹¶ä¸éš¾ç†è§£ï¼Œåœ¨`LCEL`ä¸­ï¼Œæ‰€æœ‰ç»„ä»¶éƒ½å®ç°äº†`runable`æ¥å£ï¼Œè€Œ`runable`çš„æ¥å£åè®®ä¼šå°†æ¯ä¸ªç»„ä»¶çš„è¾“å…¥å’Œè¾“å‡ºåšä¸€ä¸ªæè¿°ï¼Œä¼ å…¥`pydantic`æ¨¡å‹åšä¸€ä¸ªå¼ºæ£€éªŒï¼Œå¦‚æœä¸€åˆ‡æ­£å¸¸åˆ™ä¼šæ­£å¸¸ä¼ é€’æ•°æ®ï¼Œå¦åˆ™ç¨‹åºåˆ™æŠ›å‡ºå¼‚å¸¸ã€‚

# 3. å®ç°å¤æ‚RAGèŠå¤©æœºå™¨äºº

&emsp;&emsp;æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨ `LangChain` å’Œ `DeepSeek v3`æ¨¡å‹å¦‚ä½•æ„å»ºä¸€ä¸ªå¤æ‚çš„ `RAG` èŠå¤©æœºå™¨äººï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„æŸ¥è¯¢ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡èŠå¤©å†å²è®°å½•ç»´æŠ¤ä¸Šä¸‹æ–‡ï¼Œå¹¶ä½¿ç”¨ `LangChain` çš„ `LCEL`è¯­æ³•éµå®ˆä¸¥æ ¼çš„`Guardrails`ï¼ˆæŠ¤æ ï¼‰ã€‚

&emsp;&emsp;`Guardrails`(æŠ¤æ )å¯¹äºç¡®ä¿`AI`ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§æ˜¯æ¯”è¾ƒé‡è¦çš„ã€‚é€šè¿‡è®¾å®šæ˜ç¡®çš„ç•Œé™ï¼Œæˆ‘ä»¬å¯ä»¥é˜²æ­¢å¤§æ¨¡å‹ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§çš„å†…å®¹ã€‚æ‹’ç»æœºåˆ¶ä½¿æœºå™¨äººèƒ½å¤Ÿç¤¼è²Œåœ°æ‹’ç»è¿åè¿™äº›æŠ¤æ çš„è¯·æ±‚ï¼Œä¾‹å¦‚ä¸æ•æ„Ÿä¸»é¢˜æˆ–éæ³•æ´»åŠ¨ç›¸å…³çš„è¯·æ±‚ã€‚

&emsp;&emsp;è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ™ºèƒ½HRèŠå¤©æœºå™¨äººåŠ©æ‰‹ï¼Œè¯¥æœºå™¨äººå°†èƒ½å¤Ÿåˆ©ç”¨ç§æœ‰çŸ¥è¯†åº“å›ç­”æœ‰å…³å…¬å¸æ”¿ç­–ã€ç¨‹åºå’Œç¦åˆ©çš„é—®é¢˜ã€‚

<div align=center><img src="https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20250113222612358.png" width=80%></div>

&emsp;&emsp;ä½¿ç”¨`DeepSeek v3`æ¨¡å‹ä½œä¸ºå¯¹è¯æ¨¡å‹ã€‚


```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="deepseek-chat",
                   api_key='sk-6c960cb2a3a24fa6b92f6993d5942777',
                   base_url='https://api.deepseek.com')
```

&emsp;&emsp;ä½¿ç”¨`OpenAI`çš„`Embeddings`æ¨¡å‹å°†è‡ªç„¶è¯­è¨€è½¬åŒ–æˆè¯å‘é‡çš„è¡¨ç¤ºã€‚


```python
from langchain_openai import OpenAIEmbeddings

embed = OpenAIEmbeddings(
    api_key='sk-proj-Vs1hvbqAjlKBnTS9yTrNygLSq16ElNdDuTV1ADDgfezt3b4B5oA',
    base_url='https://ai.devtool.tech/proxy/v1',     # å›½å†…ä¸­è½¬åœ°å€
    model="text-embedding-3-large" 
)
```


```python
# ! pip install faiss-cpu
```

&emsp;&emsp;æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨`FAISS`ä½œä¸ºçŸ¢é‡æ•°æ®åº“ã€‚ `FAISS` æ˜¯ `Facebook AI Research` å¼€å‘çš„ä¸€ä¸ªåº“ï¼Œç”¨äºé«˜æ•ˆç›¸ä¼¼æ€§æœç´¢å’Œå¯†é›†å‘é‡èšç±»ã€‚`LangChain`åœ¨ç¬¬ä¸‰æ–¹é›†æˆæ¨¡å—ï¼ˆLangchain_communityï¼‰ä¸­å·²ç»æ¥å…¥äº†`FAISS`å‘é‡æ•°æ®åº“ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚


```python
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document


# åŠ è½½ä¸€äº›æ¨¡æ‹Ÿçš„å‡æ•°æ®
doc1 = Document(page_content="å‘˜å·¥æ¯å¹´äº«æœ‰ä¸€å®šæ•°é‡çš„ç—…å‡ã€‚æœ‰å…³èµ„æ ¼å’Œå…·ä½“ç»†èŠ‚å¯ä»¥åœ¨å‘˜å·¥æ‰‹å†Œä¸­æ‰¾åˆ°ã€‚")
doc2 = Document(page_content="å‘˜å·¥è¯·ç—…å‡æ—¶ï¼Œå¿…é¡»é¦–å…ˆé€šçŸ¥å…¶ä¸»ç®¡å…³äºç—…æƒ…å’Œé¢„è®¡ç¼ºå‹¤æ—¶é—´ã€‚å‘˜å·¥éœ€å¡«å†™ç—…å‡ç”³è¯·è¡¨ï¼Œå¹¶æäº¤ç»™äººåŠ›èµ„æºéƒ¨é—¨æˆ–ä¸»ç®¡ã€‚")
doc3 = Document(page_content="ç—…å‡ç”³è¯·è¡¨å¯ä»¥åœ¨å…¬å¸å†…éƒ¨ç½‘æ‰¾åˆ°ã€‚è¡¨æ ¼éœ€è¦å¡«å†™å‘˜å·¥å§“åã€éƒ¨é—¨ã€ç¼ºå‹¤æ—¥æœŸå’Œç¼ºå‹¤åŸå› ç­‰ä¿¡æ¯ã€‚")


# åˆ›å»º Faiss å‘é‡å­˜å‚¨
vector_store = FAISS.from_documents([doc1, doc2, doc3], embed)

# å°†æ–‡ä»¶ä¿å­˜åˆ°æœ¬åœ°ï¼ŒåŒ…æ‹¬ï¼šå‘é‡æ•°æ®ã€ç´¢å¼•æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶
vector_store.save_local(folder_path='.')
```

&emsp;&emsp;åˆ›å»ºçŸ¢é‡æ•°æ®åº“åï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œæµ‹è¯•ï¼š


```python
# åŠ è½½æœ¬åœ°çš„Faisså‘é‡æ–‡ä»¶ï¼Œallow_dangerous_deserialization ç”¨äºæ§åˆ¶æ˜¯å¦å…è®¸åœ¨åŠ è½½å‘é‡å­˜å‚¨æ—¶è¿›è¡Œæ½œåœ¨çš„å±é™©ååºåˆ—åŒ–æ“ä½œã€‚
vector_store = FAISS.load_local(embeddings=embed, folder_path='.',allow_dangerous_deserialization=True)
 
# å°† FAISS å‘é‡å­˜å‚¨è½¬æ¢ä¸ºä¸€ä¸ª retrieverï¼ˆæ£€ç´¢å™¨ï¼‰ï¼Œå¹¶ä¸ºè¯¥æ£€ç´¢å™¨è®¾ç½®ä¸€äº›æœç´¢ç›¸å…³çš„å‚æ•°ã€‚k=1 è¡¨ç¤ºæ£€ç´¢æ—¶è¿”å› æœ€ç›¸ä¼¼çš„ 1 ä¸ªæ–‡æ¡£
retriever = vector_store.as_retriever(search_kwargs={'k': 1})

# æ‰§è¡Œç›¸ä¼¼åº¦æœç´ 
query = "è¯·é—®æˆ‘ä»¬å…¬å¸æœ‰æ²¡æœ‰ç—…å‡ï¼Ÿ"
results = retriever.invoke(query)

for doc in results:
    print(f"Content: {doc.page_content}")
```

    Content: å‘˜å·¥æ¯å¹´äº«æœ‰ä¸€å®šæ•°é‡çš„ç—…å‡ã€‚æœ‰å…³èµ„æ ¼å’Œå…·ä½“ç»†èŠ‚å¯ä»¥åœ¨å‘˜å·¥æ‰‹å†Œä¸­æ‰¾åˆ°ã€‚


&emsp;&emsp;æˆ‘ä»¬ä»ä¸€ä¸ªæœ€ç®€å•çš„é“¾å¼€å§‹ï¼Œåªæ¥å—ç”¨æˆ·é—®é¢˜ï¼Œåœ¨æç¤ºä¸­æ ¼å¼åŒ–å®ƒå¹¶è¾“å‡ºè¯¥é—®é¢˜çš„ç­”æ¡ˆï¼ˆä¸æ£€ç´¢ï¼‰ã€‚è¿™é‡Œä½¿ç”¨ `Langchain` çš„`PromptTemplate`å¹¶ä½¿ç”¨`LCEL`å¯¹å…¶è¿›è¡Œç®¡é“ä¼ è¾“ã€‚


```python
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser

# å®šä¹‰æç¤ºæ¨¡æ¿
prompt = PromptTemplate(
  input_variables = ["question"],
  template = "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©ç†ã€‚æ“…é•¿æ ¹æ®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜ç»™å‡ºä¸€ä¸ªç®€çŸ­çš„å›ç­”ï¼š: {question}"
)


# æ„å»ºChains
chain = (
  prompt
  | model
  | StrOutputParser()
)
print(chain.invoke({"question": "è¯·é—®ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}))
```

    äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç³»ç»Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ç­‰ã€‚AIå¯ä»¥åˆ†ä¸ºå¼±äººå·¥æ™ºèƒ½ï¼ˆä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼‰å’Œå¼ºäººå·¥æ™ºèƒ½ï¼ˆå…·å¤‡é€šç”¨æ™ºèƒ½ï¼Œèƒ½åƒäººç±»ä¸€æ ·å¤„ç†å„ç§ä»»åŠ¡ï¼‰ã€‚å¸¸è§çš„AIåº”ç”¨åŒ…æ‹¬è¯­éŸ³åŠ©æ‰‹ã€å›¾åƒè¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚


&emsp;&emsp;åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¼šå°†å¸¦æœ‰`question`é”®çš„å­—å…¸è¢«ä¼ é€’åˆ°æç¤ºæ¨¡æ¿ä¸­ï¼Œå…¶ä¸­`question`å€¼è¢«æå–å¹¶åœ¨æ¨¡æ¿ä¸­æ ¼å¼åŒ–ï¼Œç„¶åä½œä¸ºè¾“å…¥ä¼ é€’åˆ°`model`ï¼Œæœ€åå°†ç»“æœæå–ä¸ºä½¿ç”¨`StrOutputPaser()`æœ€ç»ˆè¾“å‡ºå­—ç¬¦ä¸²ã€‚

&emsp;&emsp;æ¥ä¸‹æ¥ï¼Œå› ä¸ºæœ€ç»ˆæˆ‘ä»¬æƒ³è¦æ„å»ºä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œæ‰€ä»¥éœ€è¦è®©å®ƒæ”¯æŒèŠå¤©å†å²è®°å½•ï¼Œä½œä¸º`RAG`ç³»ç»Ÿçš„ä¸€ä¸ªåŸºç¡€ç»„ä»¶ã€‚å½“è°ƒç”¨é“¾æ—¶ï¼Œä»¥åˆ—è¡¨çš„å½¢å¼ä¼ é€’å†å²è®°å½•ï¼ŒæŒ‡å®šæ¯æ¡æ¶ˆæ¯æ˜¯ç”±ç”¨æˆ·è¿˜æ˜¯åŠ©æ‰‹å‘é€çš„ã€‚ä¾‹å¦‚ï¼š

```python
ï¼» {"role": "user", "content": "æˆ‘æ¯å¹´å¯ä»¥è¯·å¤šå°‘å¤©ç—…å‡ï¼Ÿ"}ï¼Œ
   {"role": "assistant", "content": "ä½ æ¯å¹´å¯ä»¥è¯·çš„ç—…å‡å¤©æ•°å–å†³äºä½ çš„å…·ä½“é›‡ä½£åˆåŒå’Œå…¬å¸æ”¿ç­–ã€‚ç„¶è€Œï¼Œä¸€èˆ¬æ¥è¯´ï¼Œå‘˜å·¥æœ‰æƒäº«å—ä¸€å®šçš„ç—…å‡ã€‚å…·ä½“ç»†èŠ‚è¯·å‚é˜…å‘˜å·¥æ‰‹å†Œæˆ–ä¸äººåŠ›èµ„æºéƒ¨é—¨è”ç³»ã€‚"},
   {"role": "user", "content": "æˆ‘åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°å‘˜å·¥æ‰‹å†Œï¼Ÿ"}ï¼Œ
   {"role": "assistant", "contentâ€œ: â€å‘˜å·¥æ‰‹å†Œé€šå¸¸åœ¨å…¬å¸å†…éƒ¨ç½‘ä¸Šæä¾›ã€‚ä½ ä¹Ÿå¯ä»¥è”ç³»ä½ çš„äººåŠ›èµ„æºéƒ¨é—¨ç´¢è¦ä¸€ä»½å®ä½“å‰¯æœ¬ã€‚"}
ï¼½
```

&emsp;&emsp;ç„¶ååˆ›å»ºé“¾ç»„ä»¶ï¼Œå°†æ­¤è¾“å…¥è½¬æ¢ä¸ºä¼ é€’ç»™`prompt_with_history`çš„è¾“å…¥ã€‚ä¸ä¸Šé¢çš„ä»£ç ç±»ä¼¼ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª `RunnableLambda`ï¼Œå®ƒç”¨æ¥è·å–æ¶ˆæ¯åˆ—è¡¨å¹¶ä»ä¸­æå–é—®é¢˜å’Œå†å²è®°å½•ã€‚ç„¶åä½¿ç”¨ `LangChain LCEL` ä¸ºå˜é‡`é—®é¢˜`åˆ†é…ä¸€ä¸ªç®¡é“ï¼Œè¯¥ç®¡é“é¦–å…ˆä»å­—å…¸ä¸­æå–å…³é”®æ¶ˆæ¯ã€‚


```python
from langchain.schema.runnable import RunnableLambda
from operator import itemgetter

# é—®é¢˜æ˜¯å†å²è®°å½•ä¸­çš„æœ€åä¸€é¡¹
def extract_question(input):
    return input[-1]["content"]

# å†å²è®°å½•æ˜¯é™¤äº†æœ€åä¸€ä¸ªé—®é¢˜ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹
def extract_history(input):
    return input[:-1]


prompt_with_history_str = """
ä½ æ˜¯ä¸€ä¸ªäººåŠ›èµ„æºåŠ©ç†èŠå¤©æœºå™¨äººã€‚è¯·åªå›ç­”HRç›¸å…³é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“æˆ–è€…è¿™ä¸ªé—®é¢˜ä¸äººåŠ›èµ„æºæ— å…³ï¼Œå°±ä¸è¦å›ç­”ã€‚
è¿™æ˜¯ä½ ä¸ç”¨æˆ·å¯¹è¯çš„å†å²è®°å½•: {chat_history}
ç°åœ¨ï¼Œè¯·å›ç­”è¿™ä¸ªé—®é¢˜: {question}
"""

# æ„å»ºæç¤ºæ¨¡æ¿
prompt_with_history = PromptTemplate(
  input_variables = ["chat_history", "question"],
  template = prompt_with_history_str
)


# æ„å»ºå¸¦æœ‰å†å²ä¼šè¯è®°å½•çš„é“¾
chain_with_history = (
    {
        # Itemgetterï¼šä»è¾“å…¥å­—å…¸ä¸­æå–ç‰¹å®šé”®ï¼Œè¿™é‡ŒæŒ‡å®šçš„æ˜¯ messages åˆ—è¡¨
        # è‡ªå®šä¹‰ lambda å‡½æ•°å¯ç”¨äºè¿›ä¸€æ­¥å¤„ç†æå–çš„æ•°æ®ï¼Œä»messagesåˆ—è¡¨ä¸­æå–questionå’Œchat_history 
        "question": itemgetter("messages") | RunnableLambda(extract_question), 
        "chat_history": itemgetter("messages") | RunnableLambda(extract_history),
    }
    | prompt_with_history
    | model
    | StrOutputParser()
)

print(chain_with_history.invoke({
    "messages": [
        {"role": "user", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"},
        {"role": "assistant", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–å…è®¸å‘˜å·¥æ¯å¹´è¯·ä¸€å®šå¤©æ•°çš„ç—…å‡ã€‚è¯¦æƒ…åŠèµ„æ ¼æ ‡å‡†è¯·å‚é˜…å‘˜å·¥æ‰‹å†Œã€‚"},
        {"role": "user", "content": "å¦‚ä½•æäº¤ç—…å‡è¯·æ±‚ï¼Ÿ"}
    ]
}))
```

    è¦æäº¤ç—…å‡è¯·æ±‚ï¼Œé€šå¸¸éœ€è¦éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
    
    1. **é€šçŸ¥ä¸»ç®¡**ï¼šé¦–å…ˆï¼Œå°½å¿«é€šçŸ¥ä½ çš„ç›´å±ä¸»ç®¡æˆ–ç»ç†ï¼Œå‘ŠçŸ¥ä»–ä»¬ä½ éœ€è¦è¯·ç—…å‡çš„æƒ…å†µã€‚
    
    2. **å¡«å†™ç”³è¯·è¡¨**ï¼šæ ¹æ®å…¬å¸æ”¿ç­–ï¼Œå¯èƒ½éœ€è¦å¡«å†™ç—…å‡ç”³è¯·è¡¨ã€‚è¿™ä¸ªè¡¨æ ¼é€šå¸¸å¯ä»¥åœ¨å…¬å¸å†…éƒ¨ç³»ç»Ÿæˆ–äººåŠ›èµ„æºéƒ¨é—¨è·å–ã€‚
    
    3. **æä¾›åŒ»ç–—è¯æ˜**ï¼šå¦‚æœå…¬å¸è¦æ±‚ï¼Œä½ å¯èƒ½éœ€è¦æä¾›åŒ»ç”Ÿå¼€å…·çš„ç—…å‡è¯æ˜æˆ–åŒ»ç–—è¯æ˜ã€‚
    
    4. **æäº¤ç”³è¯·**ï¼šå°†å¡«å†™å¥½çš„ç”³è¯·è¡¨å’Œå¿…è¦çš„è¯æ˜æ–‡ä»¶æäº¤ç»™äººåŠ›èµ„æºéƒ¨é—¨æˆ–é€šè¿‡å…¬å¸æŒ‡å®šçš„ç³»ç»Ÿæäº¤ã€‚
    
    5. **ç­‰å¾…æ‰¹å‡†**ï¼šæäº¤åï¼Œç­‰å¾…äººåŠ›èµ„æºéƒ¨é—¨æˆ–ä¸»ç®¡çš„æ‰¹å‡†ã€‚ä¸€æ—¦æ‰¹å‡†ï¼Œä½ çš„ç—…å‡è¯·æ±‚å°†è¢«æ­£å¼è®°å½•ã€‚
    
    6. **è·Ÿè¿›**ï¼šåœ¨ç—…å‡æœŸé—´ï¼Œä¿æŒä¸ä¸»ç®¡æˆ–äººåŠ›èµ„æºéƒ¨é—¨çš„æ²Ÿé€šï¼ŒåŠæ—¶æ›´æ–°ä½ çš„å¥åº·çŠ¶å†µå’Œé¢„è®¡è¿”å›å·¥ä½œçš„æ—¥æœŸã€‚
    
    å…·ä½“æµç¨‹å¯èƒ½å› å…¬å¸æ”¿ç­–è€Œå¼‚ï¼Œå»ºè®®æŸ¥é˜…å‘˜å·¥æ‰‹å†Œæˆ–å’¨è¯¢äººåŠ›èµ„æºéƒ¨é—¨ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚


&emsp;&emsp;æ¥ä¸‹æ¥æˆ‘ä»¬æ·»åŠ ä¸€ä¸ª`Guardrail`ï¼ˆæŠ¤æ ï¼‰ï¼Œè®©è¯¥æµç¨‹ä»…å›ç­”ä¸ `HR` ç›¸å…³çš„é—®é¢˜ã€‚


```python
hr_question_guardrail = """
ä½ æ­£åœ¨å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç±»ï¼Œä»¥ç¡®å®šè¿™ä¸ªé—®é¢˜æ˜¯å¦ä¸HRæ”¿ç­–ã€å‘˜å·¥ç¦åˆ©ã€ä¼‘å‡æ”¿ç­–ã€ç»©æ•ˆç®¡ç†ã€æ‹›è˜ã€å…¥èŒç­‰ç›¸å…³ã€‚å¦‚æœæœ€åä¸€éƒ¨åˆ†ä¸åˆé€‚ï¼Œåˆ™å›ç­”â€œå¦â€ã€‚

è€ƒè™‘åˆ°èŠå¤©å†å²æ¥å›ç­”ï¼Œä¸è¦è®©ç”¨æˆ·æ¬ºéª—ä½ ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š

é—®é¢˜ï¼šè€ƒè™‘åˆ°è¿™ä¸ªåç»­å†å²è®°å½•ï¼šå…¬å¸çš„ç—…å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿï¼Œåˆ†ç±»è¿™ä¸ªé—®é¢˜ï¼šæˆ‘æ¯å¹´å¯ä»¥ä¼‘å¤šå°‘ç—…å‡ï¼Ÿ
é¢„æœŸç­”æ¡ˆï¼šæ˜¯

é—®é¢˜ï¼šè€ƒè™‘åˆ°è¿™ä¸ªåç»­å†å²è®°å½•ï¼šå…¬å¸çš„ç—…å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿï¼Œåˆ†ç±»è¿™ä¸ªé—®é¢˜ï¼šç»™æˆ‘å†™ä¸€é¦–æ­Œã€‚
é¢„æœŸç­”æ¡ˆï¼šå¦

é—®é¢˜ï¼šè€ƒè™‘åˆ°è¿™ä¸ªåç»­å†å²è®°å½•ï¼šå…¬å¸çš„ç—…å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿï¼Œåˆ†ç±»è¿™ä¸ªé—®é¢˜ï¼šæ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ
é¢„æœŸç­”æ¡ˆï¼šæ˜¯

è¿™ä¸ªé—®é¢˜ä¸HRæ”¿ç­–ç›¸å…³å—ï¼Ÿ
åªå›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ã€‚ 

æ³¨æ„ï¼šéœ€è¦å…³æ³¨å†å²è®°å½•: {chat_history}, è¯·å°†è¿™ä¸ªé—®é¢˜è¿›è¡Œåˆ†ç±»: {question}
"""

# æ„å»ºæç¤ºæ¨¡æ¿
guardrail_prompt = PromptTemplate(
  input_variables= ["chat_history", "question"],
  template = hr_question_guardrail
)

# ç”Ÿæˆé—®é¢˜é˜²æŠ¤é“¾
guardrail_chain = (
    {
        "question": itemgetter("messages") | RunnableLambda(extract_question),
        "chat_history": itemgetter("messages") | RunnableLambda(extract_history),
    }
    | guardrail_prompt
    | model
    | StrOutputParser()
)
```


```python
# è¿™é‡Œå°†ä»…å›å¤ æ˜¯æˆ–è€…å¦
classify_answer = guardrail_chain.invoke({
    "messages": [
        {"role": "user", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ?"}, 
        {"role": "assistant", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–å…è®¸å‘˜å·¥æ¯å¹´ä¼‘ä¸€å®šæ•°é‡çš„ç—…å‡ã€‚å…·ä½“çš„ç»†èŠ‚å’Œèµ„æ ¼æ ‡å‡†è¯·å‚é˜…å‘˜å·¥æ‰‹å†Œã€‚"}, 
        {"role": "user", "content": "æˆ‘æ€ä¹ˆæäº¤ç—…å‡ç”³è¯·ï¼Ÿ"}
    ]
})
```


```python
classify_answer
```




    'æ˜¯'




```python
# è¿™é‡Œå°†ä»…å›å¤ æ˜¯æˆ–è€…å¦
classify_answer = guardrail_chain.invoke({
    "messages": [
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·é—®åœ¨å—ï¼Ÿ"}, 
    ]
})

classify_answer
```




    'å¦'



&emsp;&emsp;åœ¨ç”Ÿäº§åº”ç”¨ä¸­å¼€å‘å¤§æ¨¡å‹åº”ç”¨æ—¶ï¼Œæä¾›æŸäº›é˜²æŠ¤æªæ–½ä»¥ç¡®ä¿èŠå¤©æœºå™¨äººç¬¦åˆæˆ‘ä»¬çš„æ„å›¾éå¸¸é‡è¦ã€‚è€Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä¼˜åŒ–å’Œä¸°å¯Œåº”ç”¨ï¼Œæ·»åŠ æˆ‘ä»¬çš„ `langchain` æ£€ç´¢å™¨ã€‚


```python
from langchain_community.vectorstores import FAISS

def get_retriever():
    # ä½¿ç”¨ OpenAI çš„åµŒå…¥æ¨¡å‹åˆå§‹åŒ–åµŒå…¥å¯¹è±¡
    embed = OpenAIEmbeddings(
        api_key='sk-proj-Vs1hvbqA8CWfyTrNygLSq16ElNdDu-S4-nwJfZsRVVTV1ADDgfezt3b4B5oA',
        base_url='https://ai.devtool.tech/proxy/v1',
        model="text-embedding-3-large"
    )
    
    # ä»æœ¬åœ°åŠ è½½ FAISS å‘é‡å­˜å‚¨ï¼Œå¹¶ä¸”æŒ‡å®šåµŒå…¥å¯¹è±¡
    vector_store = FAISS.load_local(embeddings=embed, folder_path='.',allow_dangerous_deserialization=True)
     
    # é…ç½®æ–‡æ¡£æ£€ç´¢ï¼Œè¿”å›æœ€ç›¸å…³çš„ 1 ä¸ªæ–‡æ¡£
    retriever = vector_store.as_retriever(search_kwargs={'k': 1})
    return retriever

# æ„å»ºæ£€ç´¢å™¨å®ä¾‹
retriever = get_retriever()

# ç”Ÿæˆæ£€ç´¢é“¾
retrieve_document_chain = (
    itemgetter("messages") 
    | RunnableLambda(extract_question)
    | retriever
)

print(retrieve_document_chain.invoke({"messages": [{"role": "user", "content": "ç—…å‡çš„HRæ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"}]}))
```

    [Document(metadata={}, page_content='å‘˜å·¥æ¯å¹´äº«æœ‰ä¸€å®šæ•°é‡çš„ç—…å‡ã€‚æœ‰å…³èµ„æ ¼å’Œå…·ä½“ç»†èŠ‚å¯ä»¥åœ¨å‘˜å·¥æ‰‹å†Œä¸­æ‰¾åˆ°ã€‚')]


&emsp;&emsp;æœ€åï¼Œæˆ‘ä»¬å®ç°å®Œæ•´çš„é“¾æ¥è¿æ¥æ£€ç´¢å™¨ã€‚

<div align=center><img src="https://muyu001.oss-cn-beijing.aliyuncs.com/img/image-20250113222612358.png" width=80%></div>


```python
from langchain.schema.runnable import RunnableBranch, RunnablePassthrough

question_with_history_and_context_str = """
ä½ æ˜¯ä¸€ä¸ªå¯ä¿¡èµ–çš„ HR æ”¿ç­–åŠ©æ‰‹ã€‚ä½ å°†å›ç­”æœ‰å…³å‘˜å·¥ç¦åˆ©ã€ä¼‘å‡æ”¿ç­–ã€ç»©æ•ˆç®¡ç†ã€æ‹›è˜ã€å…¥èŒä»¥åŠå…¶ä»–ä¸ HR ç›¸å…³çš„è¯é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“é—®é¢˜çš„ç­”æ¡ˆï¼Œä½ ä¼šè¯šå®åœ°è¯´ä½ ä¸çŸ¥é“ã€‚
é˜…è¯»è®¨è®ºä»¥è·å–ä¹‹å‰å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚åœ¨èŠå¤©è®¨è®ºä¸­ï¼Œä½ è¢«ç§°ä¸ºâ€œç³»ç»Ÿâ€ï¼Œç”¨æˆ·è¢«ç§°ä¸ºâ€œç”¨æˆ·â€ã€‚

å†å²è®°å½•: {chat_history}

ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½å¸®åŠ©ä½ å›ç­”é—®é¢˜çš„ä¸Šä¸‹æ–‡ï¼š {context}

è¯·ç›´æ¥å›ç­”ï¼Œä¸è¦é‡å¤é—®é¢˜ï¼Œä¸è¦ä»¥â€œé—®é¢˜çš„ç­”æ¡ˆæ˜¯â€ä¹‹ç±»çš„å¼€å¤´ï¼Œä¸è¦åœ¨ç­”æ¡ˆå‰åŠ ä¸Šâ€œAIâ€ï¼Œä¸è¦è¯´â€œè¿™æ˜¯ç­”æ¡ˆâ€ï¼Œä¸è¦æåŠä¸Šä¸‹æ–‡æˆ–é—®é¢˜ã€‚

æ ¹æ®è¿™ä¸ªå†å²å’Œä¸Šä¸‹æ–‡ï¼Œå›ç­”è¿™ä¸ªé—®é¢˜ï¼š {question}
"""

question_with_history_and_context_prompt = PromptTemplate(
  input_variables= ["chat_history", "context", "question"],
  template = question_with_history_and_context_str
)

def format_context(docs):
    return "\n\n".join([d.page_content for d in docs])


# å®šä¹‰ä¸ç›¸å…³çš„é“¾
irrelevant_question_chain = (
  RunnableLambda(lambda x: {"result": 'æˆ‘ä¸èƒ½å›ç­”ä¸ HR æ”¿ç­–æ— å…³çš„é—®é¢˜ã€‚'})
)

# å®šä¹‰ç›¸å…³çš„é“¾
relevant_question_chain = (
  RunnablePassthrough() 
  |
  {
    "relevant_docs": prompt | model | StrOutputParser() | retriever,
    "chat_history": itemgetter("chat_history"), 
    "question": itemgetter("question")
  }
 |
  {
    "context": itemgetter("relevant_docs") | RunnableLambda(format_context),
    "chat_history": itemgetter("chat_history"), 
    "question": itemgetter("question")
  }
  |
  {
    "prompt": question_with_history_and_context_prompt,         
  }
  |
  {
    "result": itemgetter("prompt") | model | StrOutputParser(),
  }
)


# å®šä¹‰åˆ†æ”¯
branch_node = RunnableBranch(
  (lambda x: "æ˜¯" in x["question_is_relevant"].lower(), relevant_question_chain),
  (lambda x: "å¦" in x["question_is_relevant"].lower(), irrelevant_question_chain),
  irrelevant_question_chain
)

full_chain = (
  {
    "question_is_relevant": guardrail_chain,
    "question": itemgetter("messages") | RunnableLambda(extract_question),
    "chat_history": itemgetter("messages") | RunnableLambda(extract_history),    
  }
  | branch_node
)
```


```python
import json

non_relevant_dialog = {
    "messages": [
        {"role": "user", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"},
        {"role": "assistant", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–å…è®¸å‘˜å·¥æ¯å¹´ä¼‘ä¸€å®šæ•°é‡çš„ç—…å‡ã€‚å…·ä½“çš„ç»†èŠ‚å’Œèµ„æ ¼æ ‡å‡†è¯·å‚é˜…å‘˜å·¥æ‰‹å†Œã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±å‘€ã€‚"}
    ]
}

print(f'ç”¨ä¸ç›¸å…³çš„é—®é¢˜æµ‹è¯•')
response = full_chain.invoke(non_relevant_dialog)
```

    ç”¨ä¸ç›¸å…³çš„é—®é¢˜æµ‹è¯•



```python
response
```




    {'result': 'æˆ‘ä¸èƒ½å›ç­”ä¸ HR æ”¿ç­–æ— å…³çš„é—®é¢˜ã€‚'}




```python
dialog = {
    "messages": [
        {"role": "user", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"},
        {"role": "assistant", "content": "å…¬å¸çš„ç—…å‡æ”¿ç­–å…è®¸å‘˜å·¥æ¯å¹´ä¼‘ä¸€å®šæ•°é‡çš„ç—…å‡ã€‚å…·ä½“çš„ç»†èŠ‚å’Œèµ„æ ¼æ ‡å‡†è¯·å‚é˜…å‘˜å·¥æ‰‹å†Œã€‚"},
        {"role": "user", "content": "æˆ‘åº”è¯¥å¦‚ä½•æäº¤ç—…å‡çš„ç”³è¯·ï¼Ÿ"}
    ]
}
```


```python
print(retrieve_document_chain.invoke({"messages": [{"role": "user", "content": "æˆ‘åº”è¯¥å¦‚ä½•æäº¤ç—…å‡çš„ç”³è¯·ï¼Ÿ?"}]}))
```

    [Document(metadata={}, page_content='å‘˜å·¥è¯·ç—…å‡æ—¶ï¼Œå¿…é¡»é¦–å…ˆé€šçŸ¥å…¶ä¸»ç®¡å…³äºç—…æƒ…å’Œé¢„è®¡ç¼ºå‹¤æ—¶é—´ã€‚å‘˜å·¥éœ€å¡«å†™ç—…å‡ç”³è¯·è¡¨ï¼Œå¹¶æäº¤ç»™äººåŠ›èµ„æºéƒ¨é—¨æˆ–ä¸»ç®¡ã€‚')]



```python
print(f'ç”¨ç›¸å…³çš„é—®é¢˜æµ‹è¯•')
response = full_chain.invoke(dialog)
```

    ç”¨ç›¸å…³çš„é—®é¢˜æµ‹è¯•



```python
response
```




    {'result': 'ä½ åº”è¯¥é¦–å…ˆé€šçŸ¥ä½ çš„ä¸»ç®¡å…³äºç—…æƒ…å’Œé¢„è®¡ç¼ºå‹¤æ—¶é—´ã€‚ç„¶åï¼Œå¡«å†™ç—…å‡ç”³è¯·è¡¨å¹¶æäº¤ç»™äººåŠ›èµ„æºéƒ¨é—¨æˆ–ä½ çš„ä¸»ç®¡ã€‚'}



&emsp;&emsp;ä½“éªŒè¯¾æ—¶é—´æœ‰é™ï¼Œè‹¥æƒ³æ·±åº¦å­¦ä¹ å¤§æ¨¡å‹æŠ€æœ¯ï¼Œæ¬¢è¿å¤§å®¶æŠ¥åç”±æˆ‘ä¸»è®²çš„[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹](https://whakv.xetslk.com/s/3xFEAA)ï¼š

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/a4f3bdfb511aab72c83ee7d1fbefeff.png" alt="a4f3bdfb511aab72c83ee7d1fbefeff" style="zoom:30%;" />

**[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹](https://whakv.xetslk.com/s/3xFEAA)ä¸ºã€100+å°æ—¶ã€‘ä½“ç³»å¤§è¯¾ï¼Œæ€»å…±20å¤§æ¨¡å—ç²¾è®²ç²¾æï¼Œé›¶åŸºç¡€ç›´è¾¾å¤§æ¨¡å‹ä¼ä¸šçº§åº”ç”¨ï¼**

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/bbf269a0de3abba3e4ee21b29bc7cfc.png" alt="bbf269a0de3abba3e4ee21b29bc7cfc" style="zoom:50%;" />

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/eb29b5d74f0b1d86aea1b5698f6ea9d.png" alt="eb29b5d74f0b1d86aea1b5698f6ea9d" style="zoom:50%;" />

**[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹](https://whakv.xetslk.com/s/3xFEAA)2025å¹´æ–°æ˜¥ç­ä¸Šæ–°ç‰¹æƒ ï¼Œæ—©é¸Ÿä»·å…¥å­¦ï¼Œç›´é™2000ï¼Œæœ¨ç¾½è€å¸ˆç›´æ’­é—´ä¸“å±å åŠ ä¼˜æƒ åˆ¸1000ï¼Œä»…éœ€2999æŠ¥åå³å­¦ï¼Œã€ä»…é™å‰10åã€‘<span style="color:red;">è¯¦ç»†ä¿¡æ¯æ‰«ç æ·»åŠ åŠ©æ•™ï¼Œå›å¤â€œå¤§æ¨¡å‹â€ï¼Œå³å¯é¢†å–è¯¾ç¨‹å¤§çº²&æŸ¥çœ‹è¯¾ç¨‹è¯¦æƒ…ğŸ‘‡</span>**

<center><img src="https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202501131753098.png" alt="c6847a817fd7efd0cddcb1bcac217c3" style="zoom:85%;" />

<center><img src="https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202501141145823.png" alt="image-20250107200452887" style="zoom:85%;" />

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107200502389.png" alt="image-20250107200502389" style="zoom:85%;" />
